# blood_demand_forecasting.py - VERSION AVEC VRAIES DONN√âES DB
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
import joblib
from datetime import datetime, timedelta
import warnings
import time
from django.core.cache import cache
from django.db.models import Q, Sum, Avg, Count
import logging

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

# Imports conditionnels optimis√©s
try:
    import xgboost as xgb

    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    logger.info("XGBoost not available, using fallback models")

try:
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.seasonal import STL
    from statsmodels.tsa.stattools import adfuller

    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
    logger.info("Statsmodels not available, using ML models only")


class RealDataBloodDemandForecaster:
    """
    üèÜ FORECASTER AVEC VRAIES DONN√âES DB
    Toutes les donn√©es synth√©tiques supprim√©es - utilise uniquement les donn√©es r√©elles
    """

    def __init__(self, max_execution_time=120):
        self.max_execution_time = max_execution_time
        self.start_time = None

        # Mod√®les ML optimis√©s
        self.models = {
            'random_forest': RandomForestRegressor(
                n_estimators=50,  # Augment√© pour plus de pr√©cision avec vraies donn√©es
                max_depth=8,
                random_state=42,
                n_jobs=1
            )
        }

        if XGBOOST_AVAILABLE:
            self.models['xgboost'] = xgb.XGBRegressor(
                n_estimators=50,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                n_jobs=1,
                verbosity=0
            )

        self.scaler = StandardScaler()
        self.trained_models = {}
        self.model_performance = {}
        self.arima_models = {}

        # Configuration des groupes sanguins (sans donn√©es factices)
        self.blood_type_config = {
            'O+': {'priority': 'critical', 'typical_weekend_factor': 0.7},
            'A+': {'priority': 'high', 'typical_weekend_factor': 0.75},
            'B+': {'priority': 'medium', 'typical_weekend_factor': 0.8},
            'AB+': {'priority': 'low', 'typical_weekend_factor': 0.85},
            'O-': {'priority': 'critical', 'typical_weekend_factor': 0.6},
            'A-': {'priority': 'high', 'typical_weekend_factor': 0.7},
            'B-': {'priority': 'medium', 'typical_weekend_factor': 0.75},
            'AB-': {'priority': 'critical', 'typical_weekend_factor': 0.8}
        }

    def check_timeout(self):
        """V√©rifier si on approche du timeout"""
        if self.start_time and time.time() - self.start_time > self.max_execution_time:
            raise TimeoutException("Maximum execution time exceeded")

    def get_historical_data_from_db(self, blood_type, days_back=180):
        """
        üóÑÔ∏è R√âCUP√âRATION DES VRAIES DONN√âES DEPUIS LA DB
        """
        from inventory.models import BloodInventory, Transaction

        try:
            end_date = datetime.now().date()
            start_date = end_date - timedelta(days=days_back)

            logger.info(f"üìä R√©cup√©ration donn√©es DB pour {blood_type} ({start_date} √† {end_date})")

            # R√©cup√©rer les transactions par jour (demande r√©elle)
            daily_demand = Transaction.objects.filter(
                blood_type=blood_type,
                transaction_type='OUT',  # Sorties = demande
                date__range=[start_date, end_date]
            ).extra(
                select={'day': 'DATE(date)'}
            ).values('day').annotate(
                total_demand=Sum('quantity')
            ).order_by('day')

            if not daily_demand.exists():
                logger.warning(f"‚ùå Aucune donn√©e trouv√©e pour {blood_type}")
                return None

            # Convertir en DataFrame pandas
            df_data = []
            for record in daily_demand:
                df_data.append({
                    'date': record['day'],
                    'demand': record['total_demand'] or 0
                })

            df = pd.DataFrame(df_data)
            df['date'] = pd.to_datetime(df['date'])
            df = df.set_index('date')

            # Remplir les jours manquants avec 0
            idx = pd.date_range(start_date, end_date, freq='D')
            df = df.reindex(idx, fill_value=0)
            df.index.name = 'date'

            logger.info(f"‚úÖ Donn√©es r√©cup√©r√©es: {len(df)} jours, demande moyenne: {df['demand'].mean():.1f}")

            return df

        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration donn√©es DB: {e}")
            return None

    def get_contextual_data(self, blood_type):
        """
        üìà R√âCUP√âRATION DE DONN√âES CONTEXTUELLES
        Stock actuel, tendances r√©centes, etc.
        """
        from inventory.models import BloodInventory, Transaction

        try:
            # Stock actuel
            current_stock = BloodInventory.objects.filter(
                blood_type=blood_type
            ).aggregate(
                total_units=Sum('units_available'),
                avg_expiry_days=Avg('days_until_expiry')
            )

            # Tendance des 7 derniers jours
            recent_demand = Transaction.objects.filter(
                blood_type=blood_type,
                transaction_type='OUT',
                date__gte=datetime.now() - timedelta(days=7)
            ).aggregate(
                total_demand=Sum('quantity'),
                avg_daily=Avg('quantity'),
                transaction_count=Count('id')
            )

            # Tendance des 30 derniers jours
            monthly_trend = Transaction.objects.filter(
                blood_type=blood_type,
                transaction_type='OUT',
                date__gte=datetime.now() - timedelta(days=30)
            ).aggregate(
                total_demand=Sum('quantity'),
                avg_daily=Avg('quantity')
            )

            return {
                'current_stock': current_stock['total_units'] or 0,
                'avg_expiry_days': current_stock['avg_expiry_days'] or 30,
                'recent_weekly_demand': recent_demand['total_demand'] or 0,
                'recent_daily_avg': recent_demand['avg_daily'] or 0,
                'monthly_daily_avg': monthly_trend['avg_daily'] or 0,
                'recent_transactions': recent_demand['transaction_count'] or 0
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur donn√©es contextuelles: {e}")
            return {}

    def prepare_ml_features_from_real_data(self, df, contextual_data=None):
        """
        üõ†Ô∏è FEATURES ENGINEERING SUR VRAIES DONN√âES
        """
        if df is None or len(df) < 7:
            logger.warning("Donn√©es insuffisantes pour feature engineering")
            return None

        df = df.copy()

        # Features temporelles de base
        df['day_of_week'] = df.index.dayofweek
        df['month'] = df.index.month
        df['day_of_month'] = df.index.day
        df['quarter'] = df.index.quarter
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        df['is_monday'] = (df['day_of_week'] == 0).astype(int)
        df['is_friday'] = (df['day_of_week'] == 4).astype(int)

        # Moyennes mobiles sur vraies donn√©es
        for window in [3, 7, 14, 30]:
            if len(df) >= window:
                df[f'demand_ma_{window}'] = df['demand'].rolling(window=window, min_periods=1).mean()

        # Lags essentiels
        for lag in [1, 2, 7, 14]:
            if len(df) > lag:
                df[f'demand_lag_{lag}'] = df['demand'].shift(lag)

        # Tendances calcul√©es sur vraies donn√©es
        if len(df) >= 14:
            df['demand_trend_7'] = df['demand'].rolling(7, min_periods=3).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 2 else 0
            )
            df['demand_trend_14'] = df['demand'].rolling(14, min_periods=7).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 2 else 0
            )

        # Volatilit√© r√©cente
        if len(df) >= 7:
            df['demand_volatility_7'] = df['demand'].rolling(7, min_periods=3).std()

        # Features cycliques
        df['sin_day_of_week'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
        df['cos_day_of_week'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
        df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)
        df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)

        # Features contextuelles si disponibles
        if contextual_data:
            df['stock_ratio'] = contextual_data.get('current_stock', 0) / max(1, df['demand'].mean())
            df['recent_trend_factor'] = contextual_data.get('recent_daily_avg', 0) / max(1, df['demand'].mean())

        return df

    def train_model_with_real_data(self, blood_type, method='auto'):
        """
        üéØ ENTRA√éNEMENT AVEC VRAIES DONN√âES UNIQUEMENT
        """
        self.start_time = time.time()

        # Cache intelligent
        cache_key = f'real_model_{blood_type}_{method}'
        cached_result = cache.get(cache_key)
        if cached_result:
            logger.info(f"‚úÖ Mod√®le en cache pour {blood_type}")
            self.model_performance[blood_type] = cached_result['performance']
            self.trained_models.update(cached_result['models'])
            return cached_result['performance'], cached_result['best_method']

        # R√©cup√©rer les vraies donn√©es
        historical_data = self.get_historical_data_from_db(blood_type)
        if historical_data is None or len(historical_data) < 14:
            logger.error(f"‚ùå Donn√©es insuffisantes pour {blood_type}")
            return {}, 'insufficient_data'

        # Donn√©es contextuelles
        contextual_data = self.get_contextual_data(blood_type)

        logger.info(f"üî¨ Entra√Ænement mod√®le pour {blood_type} avec {len(historical_data)} jours de vraies donn√©es")

        results = {}

        try:
            # Auto-s√©lection de m√©thode bas√©e sur les vraies donn√©es
            if method == 'auto':
                method = self.select_optimal_method_for_real_data(historical_data, blood_type)

            # Entra√Ænement selon la m√©thode choisie
            if method == 'random_forest' or method == 'xgboost':
                results = self.train_ml_models_real_data(historical_data, blood_type, contextual_data, method)

            elif method == 'arima' and STATSMODELS_AVAILABLE:
                results['arima'] = self.train_arima_real_data(historical_data, blood_type)

            elif method == 'stl_arima' and STATSMODELS_AVAILABLE:
                results['stl_arima'] = self.train_stl_arima_real_data(historical_data, blood_type)

            # S√©lection du meilleur mod√®le
            if results:
                best_method = min(results.items(), key=lambda x: x[1].get('mape', float('inf')))[0]

                # Cache des r√©sultats
                cache_data = {
                    'performance': results,
                    'models': {k: v for k, v in self.trained_models.items() if blood_type in k},
                    'best_method': best_method,
                    'data_points': len(historical_data),
                    'contextual_data': contextual_data
                }
                cache.set(cache_key, cache_data, 3600)  # Cache 1 heure

                self.model_performance[blood_type] = results
                logger.info(f"‚úÖ Mod√®le entra√Æn√©: {best_method} (MAPE: {results[best_method].get('mape', 0):.2f}%)")

                return results, best_method
            else:
                logger.warning(f"‚ö†Ô∏è Aucun mod√®le n'a pu √™tre entra√Æn√© pour {blood_type}")
                return {}, 'training_failed'

        except Exception as e:
            logger.error(f"‚ùå Erreur entra√Ænement pour {blood_type}: {e}")
            return {}, 'error'

    def select_optimal_method_for_real_data(self, data, blood_type):
        """
        ü§ñ S√âLECTION INTELLIGENTE bas√©e sur les caract√©ristiques des vraies donn√©es
        """
        try:
            series = data['demand']

            # Analyser les caract√©ristiques des vraies donn√©es
            mean_demand = series.mean()
            volatility = series.std() / max(mean_demand, 1)
            trend_strength = abs(np.corrcoef(range(len(series)), series)[0, 1]) if len(series) > 10 else 0

            # D√©tection de saisonnalit√© r√©elle
            if len(series) >= 14:
                from scipy import stats
                weekly_pattern = [series[series.index.dayofweek == i].mean() for i in range(7)]
                seasonality_strength = np.std(weekly_pattern) / max(np.mean(weekly_pattern), 1)
            else:
                seasonality_strength = 0

            logger.info(f"üìä Analyse donn√©es {blood_type}: volatilit√©={volatility:.2f}, "
                        f"tendance={trend_strength:.2f}, saisonnalit√©={seasonality_strength:.2f}")

            # Logique de s√©lection bas√©e sur les donn√©es r√©elles
            if seasonality_strength > 0.3 and len(series) >= 21 and STATSMODELS_AVAILABLE:
                return 'stl_arima'  # Forte saisonnalit√© d√©tect√©e
            elif trend_strength > 0.5 and STATSMODELS_AVAILABLE:
                return 'arima'  # Forte tendance d√©tect√©e
            elif volatility < 0.5 and XGBOOST_AVAILABLE:
                return 'xgboost'  # Donn√©es stables, ML performant
            else:
                return 'random_forest'  # Cas g√©n√©ral robuste

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur s√©lection m√©thode: {e}")
            return 'random_forest'  # Fallback s√ªr

    def train_ml_models_real_data(self, data, blood_type, contextual_data, method):
        """
        ü§ñ ENTRA√éNEMENT ML SUR VRAIES DONN√âES
        """
        try:
            # Features engineering sur vraies donn√©es
            df_features = self.prepare_ml_features_from_real_data(data, contextual_data)
            if df_features is None:
                return {}

            df_features = df_features.dropna()

            if len(df_features) < 10:
                logger.warning(f"‚ö†Ô∏è Pas assez de donn√©es apr√®s nettoyage: {len(df_features)}")
                return {}

            # S√©lection des features
            feature_cols = [col for col in df_features.columns
                            if col not in ['demand'] and not col.startswith('demand_ratio')]

            X = df_features[feature_cols]
            y = df_features['demand']

            # Split temporel (important pour les s√©ries temporelles)
            split_idx = max(7, int(len(df_features) * 0.8))
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]

            results = {}

            # Entra√Æner le mod√®le sp√©cifi√©
            if method in ['random_forest', 'auto'] or (method == 'xgboost' and not XGBOOST_AVAILABLE):
                model = self.models['random_forest']
                model.fit(X_train, y_train)
                pred = model.predict(X_test)

                results['random_forest'] = {
                    'mae': float(mean_absolute_error(y_test, pred)),
                    'rmse': float(np.sqrt(mean_squared_error(y_test, pred))),
                    'mape': float(mean_absolute_percentage_error(y_test, pred) * 100),
                    'training_samples': len(X_train),
                    'test_samples': len(X_test)
                }

                self.trained_models[f'rf_{blood_type}'] = {
                    'model': model,
                    'features': feature_cols,
                    'scaler': None,
                    'trained_date': datetime.now()
                }

                logger.info(f"‚úÖ Random Forest: MAPE {results['random_forest']['mape']:.2f}%")

            if method == 'xgboost' and XGBOOST_AVAILABLE:
                model = self.models['xgboost']
                model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
                pred = model.predict(X_test)

                results['xgboost'] = {
                    'mae': float(mean_absolute_error(y_test, pred)),
                    'rmse': float(np.sqrt(mean_squared_error(y_test, pred))),
                    'mape': float(mean_absolute_percentage_error(y_test, pred) * 100),
                    'training_samples': len(X_train),
                    'test_samples': len(X_test)
                }

                self.trained_models[f'xgb_{blood_type}'] = {
                    'model': model,
                    'features': feature_cols,
                    'scaler': None,
                    'trained_date': datetime.now()
                }

                logger.info(f"‚úÖ XGBoost: MAPE {results['xgboost']['mape']:.2f}%")

            return results

        except Exception as e:
            logger.error(f"‚ùå Erreur entra√Ænement ML: {e}")
            return {}

    def train_arima_real_data(self, data, blood_type):
        """
        üìà ARIMA SUR VRAIES DONN√âES
        """
        if not STATSMODELS_AVAILABLE:
            return {}

        try:
            series = data['demand']

            if len(series) < 20:
                logger.warning(f"‚ö†Ô∏è Pas assez de donn√©es pour ARIMA: {len(series)}")
                return {}

            # Auto-s√©lection de l'ordre ARIMA sur vraies donn√©es
            best_aic = float('inf')
            best_order = (1, 1, 1)

            for p in range(3):
                for d in range(2):
                    for q in range(3):
                        try:
                            model = ARIMA(series, order=(p, d, q))
                            fitted = model.fit()
                            if fitted.aic < best_aic:
                                best_aic = fitted.aic
                                best_order = (p, d, q)
                        except:
                            continue

            # Mod√®le final
            final_model = ARIMA(series, order=best_order)
            fitted_final = final_model.fit()

            # √âvaluation
            fitted_values = fitted_final.fittedvalues
            residuals = series[len(series) - len(fitted_values):] - fitted_values

            mae = float(np.mean(np.abs(residuals)))
            rmse = float(np.sqrt(np.mean(residuals ** 2)))

            # MAPE sur vraies donn√©es
            actual = series[len(series) - len(fitted_values):]
            mape = float(np.mean(np.abs((actual - fitted_values) / np.maximum(actual, 1))) * 100)

            self.arima_models[blood_type] = fitted_final

            logger.info(f"‚úÖ ARIMA {best_order}: MAPE {mape:.2f}%")

            return {
                'mae': mae,
                'rmse': rmse,
                'mape': mape,
                'order': best_order,
                'aic': float(fitted_final.aic),
                'training_samples': len(series)
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur ARIMA: {e}")
            return {}

    def train_stl_arima_real_data(self, data, blood_type):
        """
        üî¨ STL + ARIMA SUR VRAIES DONN√âES
        """
        if not STATSMODELS_AVAILABLE:
            return {}

        try:
            series = data['demand']

            if len(series) < 28:  # Au moins 4 semaines
                logger.warning(f"‚ö†Ô∏è Pas assez de donn√©es pour STL: {len(series)}")
                return {}

            # D√©composition STL sur vraies donn√©es
            stl = STL(series, seasonal=7, robust=True)  # Cycle hebdomadaire
            decomposition = stl.fit()

            # ARIMA sur r√©sidus
            deseasonalized = series - decomposition.seasonal

            # Auto-s√©lection ordre ARIMA
            best_aic = float('inf')
            best_order = (1, 0, 1)

            for p in range(3):
                for d in range(2):
                    for q in range(3):
                        try:
                            model = ARIMA(deseasonalized, order=(p, d, q))
                            fitted = model.fit()
                            if fitted.aic < best_aic:
                                best_aic = fitted.aic
                                best_order = (p, d, q)
                        except:
                            continue

            # Mod√®le final
            arima_model = ARIMA(deseasonalized, order=best_order)
            fitted_arima = arima_model.fit()

            # √âvaluation avec reconstruction
            arima_fitted = fitted_arima.fittedvalues
            reconstructed = arima_fitted + decomposition.seasonal[len(decomposition.seasonal) - len(arima_fitted):]

            actual_for_eval = series[len(series) - len(reconstructed):]
            residuals = actual_for_eval - reconstructed

            mae = float(np.mean(np.abs(residuals)))
            rmse = float(np.sqrt(np.mean(residuals ** 2)))
            mape = float(np.mean(np.abs((actual_for_eval - reconstructed) / np.maximum(actual_for_eval, 1))) * 100)

            # Sauvegarder les composantes
            self.trained_models[f'stl_{blood_type}'] = {
                'arima_model': fitted_arima,
                'seasonal_component': decomposition.seasonal,
                'trend_component': decomposition.trend,
                'order': best_order,
                'trained_date': datetime.now()
            }

            logger.info(f"‚úÖ STL+ARIMA {best_order}: MAPE {mape:.2f}%")

            return {
                'mae': mae,
                'rmse': rmse,
                'mape': mape,
                'order': best_order,
                'aic': float(fitted_arima.aic),
                'seasonal_strength': float(np.std(decomposition.seasonal)),
                'training_samples': len(series)
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur STL+ARIMA: {e}")
            return {}

    def predict_with_real_data(self, blood_type, days_ahead=7, method='auto'):
        """
        üîÆ PR√âDICTION BAS√âE SUR VRAIES DONN√âES UNIQUEMENT
        """
        cache_key = f'real_prediction_{blood_type}_{days_ahead}_{method}'
        cached = cache.get(cache_key)
        if cached:
            logger.info(f"‚úÖ Pr√©diction en cache pour {blood_type}")
            return cached

        self.start_time = time.time()

        try:
            # Entra√Æner le mod√®le avec vraies donn√©es
            performance, best_method = self.train_model_with_real_data(blood_type, method)

            if not performance:
                logger.error(f"‚ùå Impossible d'entra√Æner le mod√®le pour {blood_type}")
                return self.emergency_fallback_real_data(blood_type, days_ahead)

            # Utiliser la meilleure m√©thode trouv√©e
            final_method = best_method if method == 'auto' else method

            # G√©n√©ration des pr√©dictions
            predictions = self.generate_predictions_real_data(blood_type, days_ahead, final_method)

            if not predictions:
                return self.emergency_fallback_real_data(blood_type, days_ahead)

            # Donn√©es contextuelles pour enrichir le r√©sultat
            contextual_data = self.get_contextual_data(blood_type)

            result = {
                'blood_type': blood_type,
                'predictions': predictions,
                'method_used': final_method,
                'model_performance': performance.get(final_method, {}),
                'confidence_intervals': self.calculate_confidence_intervals_real_data(predictions),
                'generated_at': datetime.now().isoformat(),
                'data_source': 'real_database',
                'contextual_insights': {
                    'current_stock': contextual_data.get('current_stock', 0),
                    'recent_trend': contextual_data.get('recent_daily_avg', 0),
                    'stock_days_remaining': self.calculate_stock_duration(contextual_data, predictions)
                },
                'quality_metrics': {
                    'training_accuracy': performance.get(final_method, {}).get('mape', 0),
                    'data_freshness': 'real_time',
                    'prediction_confidence': self.calculate_overall_confidence(predictions,
                                                                               performance.get(final_method, {}))
                }
            }

            # Cache adaptatif selon la performance
            cache_duration = 1800 if performance.get(final_method, {}).get('mape', 100) < 20 else 900
            cache.set(cache_key, result, cache_duration)

            logger.info(f"‚úÖ Pr√©diction g√©n√©r√©e pour {blood_type} avec m√©thode {final_method}")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction: {e}")
            return self.emergency_fallback_real_data(blood_type, days_ahead)

    def generate_predictions_real_data(self, blood_type, days_ahead, method):
        """
        üéØ G√âN√âRATION DES PR√âDICTIONS avec vraies donn√©es
        """
        try:
            if method == 'random_forest' or method == 'xgboost':
                return self.predict_ml_real_data(blood_type, days_ahead, method)
            elif method == 'arima':
                return self.predict_arima_real_data(blood_type, days_ahead)
            elif method == 'stl_arima':
                return self.predict_stl_arima_real_data(blood_type, days_ahead)
            else:
                logger.warning(f"‚ö†Ô∏è M√©thode inconnue: {method}")
                return None

        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration pr√©dictions: {e}")
            return None

    def predict_ml_real_data(self, blood_type, days_ahead, method):
        """
        ü§ñ PR√âDICTION ML bas√©e sur features des vraies donn√©es
        """
        model_key = f"{'rf' if method == 'random_forest' else 'xgb'}_{blood_type}"

        if model_key not in self.trained_models:
            logger.error(f"‚ùå Mod√®le {model_key} non trouv√©")
            return None

        try:
            model_data = self.trained_models[model_key]
            model = model_data['model']
            feature_cols = model_data['features']

            # R√©cup√©rer les derni√®res donn√©es r√©elles pour construire les features futures
            recent_data = self.get_historical_data_from_db(blood_type, days_back=30)
            if recent_data is None:
                return None

            # Pr√©parer les features sur les donn√©es r√©centes
            contextual_data = self.get_contextual_data(blood_type)
            df_with_features = self.prepare_ml_features_from_real_data(recent_data, contextual_data)

            if df_with_features is None:
                return None

            predictions = []
            last_known_values = df_with_features['demand'].tail(14).values  # Derni√®res 2 semaines

            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)

                # Construction des features futures bas√©es sur les patterns r√©els
                future_features = self.build_future_features_from_real_patterns(
                    future_date, df_with_features, last_known_values, i, contextual_data
                )

                if len(future_features) != len(feature_cols):
                    logger.warning(f"‚ö†Ô∏è Mismatch features: {len(future_features)} vs {len(feature_cols)}")
                    continue

                # Pr√©diction
                pred = model.predict([future_features])[0]
                pred = max(0, int(pred))  # Pas de demande n√©gative

                # Calcul de confiance bas√© sur la variance r√©cente
                recent_variance = np.var(last_known_values[-7:]) if len(last_known_values) >= 7 else 1
                base_confidence = max(0.6, min(0.95, 1.0 - (recent_variance / max(np.mean(last_known_values), 1))))

                # Diminution de confiance avec la distance temporelle
                confidence = base_confidence * (0.98 ** i)

                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': pred,
                    'confidence': round(confidence, 3),
                    'method_details': {
                        'features_used': len(feature_cols),
                        'base_confidence': round(base_confidence, 3),
                        'temporal_decay': round(0.98 ** i, 3)
                    }
                })

                # Mettre √† jour les valeurs connues pour la pr√©diction suivante
                if len(last_known_values) >= 14:
                    last_known_values = np.append(last_known_values[1:], pred)
                else:
                    last_known_values = np.append(last_known_values, pred)

            return predictions

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction ML: {e}")
            return None

    def build_future_features_from_real_patterns(self, future_date, historical_df, last_values, day_offset,
                                                 contextual_data):
        """
        üèóÔ∏è CONSTRUCTION DE FEATURES FUTURES bas√©es sur les patterns des vraies donn√©es
        """
        try:
            features = []

            # Features temporelles de base
            features.extend([
                future_date.weekday(),  # day_of_week
                future_date.month,  # month
                future_date.day,  # day_of_month
                future_date.quarter,  # quarter
                1 if future_date.weekday() in [5, 6] else 0,  # is_weekend
                1 if future_date.weekday() == 0 else 0,  # is_monday
                1 if future_date.weekday() == 4 else 0,  # is_friday
            ])

            # Moyennes mobiles bas√©es sur les derni√®res valeurs r√©elles
            if len(last_values) >= 3:
                features.append(np.mean(last_values[-3:]))  # demand_ma_3
            else:
                features.append(historical_df['demand'].mean())

            if len(last_values) >= 7:
                features.append(np.mean(last_values[-7:]))  # demand_ma_7
            else:
                features.append(historical_df['demand'].mean())

            if len(last_values) >= 14:
                features.append(np.mean(last_values[-14:]))  # demand_ma_14
            else:
                features.append(historical_df['demand'].mean())

            # Moyenne sur 30 jours si disponible
            if 'demand_ma_30' in historical_df.columns:
                features.append(historical_df['demand_ma_30'].iloc[-1])
            else:
                features.append(historical_df['demand'].mean())

            # Lags bas√©s sur les vraies donn√©es
            for lag in [1, 2, 7, 14]:
                if len(last_values) >= lag:
                    features.append(last_values[-lag])
                else:
                    features.append(historical_df['demand'].mean())

            # Tendances calcul√©es sur les vraies donn√©es r√©centes
            if len(last_values) >= 7:
                trend_7 = np.polyfit(range(7), last_values[-7:], 1)[0]
                features.append(trend_7)
            else:
                features.append(0.0)

            if len(last_values) >= 14:
                trend_14 = np.polyfit(range(14), last_values[-14:], 1)[0]
                features.append(trend_14)
            else:
                features.append(0.0)

            # Volatilit√© r√©cente
            if len(last_values) >= 7:
                volatility = np.std(last_values[-7:])
                features.append(volatility)
            else:
                features.append(historical_df['demand'].std())

            # Features cycliques
            features.extend([
                np.sin(2 * np.pi * future_date.weekday() / 7),
                np.cos(2 * np.pi * future_date.weekday() / 7),
                np.sin(2 * np.pi * future_date.month / 12),
                np.cos(2 * np.pi * future_date.month / 12),
            ])

            # Features contextuelles
            if contextual_data:
                avg_demand = np.mean(last_values) if len(last_values) > 0 else historical_df['demand'].mean()
                features.extend([
                    contextual_data.get('current_stock', 0) / max(1, avg_demand),  # stock_ratio
                    contextual_data.get('recent_daily_avg', 0) / max(1, avg_demand)  # recent_trend_factor
                ])
            else:
                features.extend([1.0, 1.0])  # Valeurs par d√©faut

            return features

        except Exception as e:
            logger.error(f"‚ùå Erreur construction features: {e}")
            return []

    def predict_arima_real_data(self, blood_type, days_ahead):
        """
        üìà PR√âDICTION ARIMA sur vraies donn√©es
        """
        if blood_type not in self.arima_models:
            logger.error(f"‚ùå Mod√®le ARIMA non trouv√© pour {blood_type}")
            return None

        try:
            model = self.arima_models[blood_type]

            # Pr√©diction ARIMA
            forecast = model.forecast(steps=days_ahead)
            conf_int = model.get_forecast(steps=days_ahead).conf_int()

            predictions = []

            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)
                pred = max(0, int(forecast.iloc[i]))

                # Confiance bas√©e sur l'intervalle de confiance
                lower_bound = max(0, conf_int.iloc[i, 0])
                upper_bound = conf_int.iloc[i, 1]
                confidence_width = upper_bound - lower_bound

                # Normaliser la confiance (plus l'intervalle est √©troit, plus la confiance est √©lev√©e)
                base_confidence = max(0.5, min(0.95, 1.0 - (confidence_width / max(pred, 1))))
                confidence = base_confidence * (0.97 ** i)  # D√©croissance temporelle

                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': pred,
                    'confidence': round(confidence, 3),
                    'lower_bound': max(0, int(lower_bound)),
                    'upper_bound': max(pred, int(upper_bound)),
                    'method_details': {
                        'confidence_interval_width': round(confidence_width, 2),
                        'forecast_value': round(float(forecast.iloc[i]), 2)
                    }
                })

            return predictions

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction ARIMA: {e}")
            return None

    def predict_stl_arima_real_data(self, blood_type, days_ahead):
        """
        üî¨ PR√âDICTION STL + ARIMA sur vraies donn√©es
        """
        model_key = f'stl_{blood_type}'

        if model_key not in self.trained_models:
            logger.error(f"‚ùå Mod√®le STL non trouv√© pour {blood_type}")
            return None

        try:
            model_data = self.trained_models[model_key]
            arima_model = model_data['arima_model']
            seasonal_component = model_data['seasonal_component']

            # Pr√©diction de la composante de tendance
            trend_forecast = arima_model.forecast(steps=days_ahead)

            # Reconstruction avec saisonnalit√©
            seasonal_pattern = seasonal_component.tail(7).values  # Dernier pattern hebdomadaire

            predictions = []

            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)

                # Composante saisonni√®re cyclique
                seasonal_value = seasonal_pattern[i % 7]

                # Pr√©diction finale
                trend_value = trend_forecast.iloc[i]
                final_pred = max(0, int(trend_value + seasonal_value))

                # Confiance bas√©e sur la stabilit√© de la d√©composition
                seasonal_stability = 1.0 - (np.std(seasonal_pattern) / max(np.mean(seasonal_pattern), 1))
                base_confidence = max(0.6, min(0.9, seasonal_stability))
                confidence = base_confidence * (0.96 ** i)

                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': final_pred,
                    'confidence': round(confidence, 3),
                    'seasonal_component': round(seasonal_value, 2),
                    'trend_component': round(float(trend_value), 2),
                    'method_details': {
                        'seasonal_pattern_day': i % 7,
                        'seasonal_stability': round(seasonal_stability, 3)
                    }
                })

            return predictions

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction STL: {e}")
            return None

    def calculate_confidence_intervals_real_data(self, predictions):
        """
        üìä CALCUL D'INTERVALLES DE CONFIANCE bas√©s sur les vraies donn√©es
        """
        if not predictions:
            return {'lower': [], 'upper': [], 'margin': 0}

        try:
            demands = [p['predicted_demand'] for p in predictions]
            confidences = [p['confidence'] for p in predictions]

            lower_bounds = []
            upper_bounds = []

            for i, (demand, conf) in enumerate(zip(demands, confidences)):
                # Marge d'erreur adaptative
                base_margin = demand * (1.0 - conf)  # Plus la confiance est faible, plus la marge est large
                time_margin = demand * 0.05 * i  # Augmentation avec le temps

                total_margin = base_margin + time_margin

                lower_bounds.append(max(0, int(demand - total_margin)))
                upper_bounds.append(int(demand + total_margin))

            return {
                'lower': lower_bounds,
                'upper': upper_bounds,
                'margin': float(np.mean([u - d for u, d in zip(upper_bounds, demands)]))
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur calcul intervalles: {e}")
            return {'lower': [], 'upper': [], 'margin': 0}

    def calculate_stock_duration(self, contextual_data, predictions):
        """
        üì¶ CALCUL DE LA DUR√âE DE VIE DU STOCK bas√© sur les vraies pr√©dictions
        """
        try:
            current_stock = contextual_data.get('current_stock', 0)
            if current_stock <= 0 or not predictions:
                return 0

            cumulative_demand = 0
            for i, pred in enumerate(predictions):
                cumulative_demand += pred['predicted_demand']
                if cumulative_demand >= current_stock:
                    return i + 1

            # Si le stock dure plus longtemps que nos pr√©dictions
            return len(predictions) + 1

        except Exception as e:
            logger.error(f"‚ùå Erreur calcul dur√©e stock: {e}")
            return 0

    def calculate_overall_confidence(self, predictions, performance):
        """
        üéØ CALCUL DE LA CONFIANCE GLOBALE
        """
        try:
            if not predictions or not performance:
                return 0.5

            # Confiance moyenne des pr√©dictions
            pred_confidence = np.mean([p['confidence'] for p in predictions])

            # Confiance bas√©e sur la performance du mod√®le
            model_mape = performance.get('mape', 50)
            model_confidence = max(0.1, min(0.9, 1.0 - (model_mape / 100)))

            # Confiance combin√©e
            overall = (pred_confidence * 0.6) + (model_confidence * 0.4)

            return round(overall, 3)

        except Exception as e:
            logger.error(f"‚ùå Erreur calcul confiance: {e}")
            return 0.5

    def emergency_fallback_real_data(self, blood_type, days_ahead):
        """
        üö® FALLBACK D'URGENCE bas√© sur les moyennes r√©centes des vraies donn√©es
        """
        try:
            logger.warning(f"üö® Utilisation du fallback d'urgence pour {blood_type}")

            # R√©cup√©rer les donn√©es r√©centes
            recent_data = self.get_historical_data_from_db(blood_type, days_back=30)
            contextual_data = self.get_contextual_data(blood_type)

            if recent_data is not None and len(recent_data) > 0:
                # Utiliser les moyennes r√©elles r√©centes
                recent_mean = recent_data['demand'].tail(14).mean()
                recent_std = recent_data['demand'].tail(14).std()

                # Pattern hebdomadaire bas√© sur les vraies donn√©es
                weekly_pattern = []
                for day in range(7):
                    day_data = recent_data[recent_data.index.dayofweek == day]['demand']
                    if len(day_data) > 0:
                        weekly_pattern.append(day_data.mean())
                    else:
                        weekly_pattern.append(recent_mean)

                weekly_avg = np.mean(weekly_pattern) if weekly_pattern else recent_mean

            else:
                # Utiliser les donn√©es contextuelles si disponibles
                recent_mean = max(1, contextual_data.get('recent_daily_avg', 5))
                recent_std = recent_mean * 0.3
                weekly_pattern = [recent_mean] * 7
                weekly_avg = recent_mean

            predictions = []

            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)
                day_of_week = future_date.weekday()

                # Utiliser le pattern hebdomadaire r√©el
                if len(weekly_pattern) > day_of_week:
                    base_demand = weekly_pattern[day_of_week]
                else:
                    base_demand = recent_mean

                # Normaliser par rapport √† la moyenne hebdomadaire
                if weekly_avg > 0:
                    seasonal_factor = base_demand / weekly_avg
                else:
                    seasonal_factor = 1.0

                # Ajustement pour les weekends (bas√© sur les donn√©es r√©elles si disponibles)
                if day_of_week in [5, 6]:  # Weekend
                    config = self.blood_type_config.get(blood_type, {})
                    weekend_factor = config.get('typical_weekend_factor', 0.8)
                    seasonal_factor *= weekend_factor

                final_demand = max(1, int(recent_mean * seasonal_factor))

                # Confiance r√©duite pour le fallback mais pas nulle
                confidence = max(0.4, min(0.7, 0.6 - (i * 0.02)))

                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': final_demand,
                    'confidence': round(confidence, 3)
                })

            return {
                'blood_type': blood_type,
                'predictions': predictions,
                'method_used': 'emergency_fallback_real_data',
                'confidence_intervals': self.calculate_confidence_intervals_real_data(predictions),
                'generated_at': datetime.now().isoformat(),
                'data_source': 'real_database_limited',
                'warning': 'Pr√©diction de secours bas√©e sur les moyennes r√©centes r√©elles',
                'contextual_insights': {
                    'current_stock': contextual_data.get('current_stock', 0),
                    'recent_trend': contextual_data.get('recent_daily_avg', 0),
                    'data_availability': 'limited'
                }
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur fallback d'urgence: {e}")

            # Fallback ultime avec valeurs minimales
            config = self.blood_type_config.get(blood_type, {})
            min_demand = 2 if config.get('priority') == 'critical' else 1

            predictions = []
            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)
                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': min_demand,
                    'confidence': 0.3
                })

            return {
                'blood_type': blood_type,
                'predictions': predictions,
                'method_used': 'minimal_fallback',
                'generated_at': datetime.now().isoformat(),
                'warning': 'Pr√©diction minimale - donn√©es insuffisantes',
                'error': str(e)
            }

    def get_model_performance_summary(self, blood_type):
        """
        üìä R√âSUM√â DES PERFORMANCES DU MOD√àLE
        """
        try:
            if blood_type not in self.model_performance:
                return {}

            performance = self.model_performance[blood_type]

            summary = {
                'best_method': min(performance.items(), key=lambda x: x[1].get('mape', float('inf')))[0],
                'best_mape': min([p.get('mape', float('inf')) for p in performance.values()]),
                'methods_trained': list(performance.keys()),
                'training_data_points': performance.get(list(performance.keys())[0], {}).get('training_samples', 0),
                'last_training': datetime.now().isoformat()
            }

            return summary

        except Exception as e:
            logger.error(f"‚ùå Erreur r√©sum√© performance: {e}")
            return {}

    def clear_model_cache(self, blood_type=None):
        """
        üßπ NETTOYAGE DU CACHE DES MOD√àLES
        """
        try:
            if blood_type:
                # Nettoyage sp√©cifique
                cache_keys = [
                    f'real_model_{blood_type}_auto',
                    f'real_model_{blood_type}_random_forest',
                    f'real_model_{blood_type}_xgboost',
                    f'real_model_{blood_type}_arima',
                    f'real_model_{blood_type}_stl_arima',
                    f'real_prediction_{blood_type}_7_auto',
                    f'real_prediction_{blood_type}_14_auto',
                    f'real_prediction_{blood_type}_30_auto'
                ]
                cache.delete_many(cache_keys)

                # Nettoyage des mod√®les en m√©moire
                keys_to_remove = [k for k in self.trained_models.keys() if blood_type in k]
                for key in keys_to_remove:
                    del self.trained_models[key]

                if blood_type in self.model_performance:
                    del self.model_performance[blood_type]

                if blood_type in self.arima_models:
                    del self.arima_models[blood_type]

                logger.info(f"‚úÖ Cache nettoy√© pour {blood_type}")

            else:
                # Nettoyage global
                cache.clear()
                self.trained_models.clear()
                self.model_performance.clear()
                self.arima_models.clear()

                logger.info("‚úÖ Cache global nettoy√©")

        except Exception as e:
            logger.error(f"‚ùå Erreur nettoyage cache: {e}")


class TimeoutException(Exception):
    """Exception lev√©e en cas de timeout"""
    pass


# ==================== FONCTIONS D'API POUR L'INTERFACE ====================

def generate_forecast_api(blood_type, days_ahead=7, method='auto', force_retrain=False):
    """
    üéØ FONCTION PRINCIPALE D'API pour l'interface React

    Args:
        blood_type: Type de sang (O+, A+, etc.)
        days_ahead: Nombre de jours √† pr√©dire
        method: M√©thode √† utiliser ('auto', 'random_forest', 'xgboost', 'arima', 'stl_arima')
        force_retrain: Forcer le r√©entra√Ænement du mod√®le

    Returns:
        dict: R√©sultat complet de la pr√©diction
    """
    forecaster = RealDataBloodDemandForecaster()

    try:
        # Forcer le nettoyage du cache si demand√©
        if force_retrain:
            forecaster.clear_model_cache(blood_type)

        # G√©n√©rer la pr√©diction
        result = forecaster.predict_with_real_data(blood_type, days_ahead, method)

        # Enrichir avec des m√©tadonn√©es pour l'interface
        result['api_response'] = {
            'timestamp': datetime.now().isoformat(),
            'processing_time_ms': int((time.time() - time.time()) * 1000),  # √Ä ajuster
            'version': '2.0-real-data',
            'data_source': 'production_database'
        }

        # Ajouter les recommandations automatiques
        result['optimization_recommendations'] = generate_recommendations(result)

        return result

    except Exception as e:
        logger.error(f"‚ùå Erreur API g√©n√©ration pr√©vision: {e}")
        return {
            'error': True,
            'message': str(e),
            'blood_type': blood_type,
            'method_attempted': method,
            'timestamp': datetime.now().isoformat()
        }


def generate_recommendations(forecast_result):
    """
    üí° G√âN√âRATION DE RECOMMANDATIONS bas√©es sur les vraies pr√©dictions
    """
    try:
        if not forecast_result.get('predictions'):
            return []

        recommendations = []
        predictions = forecast_result['predictions']
        blood_type = forecast_result['blood_type']

        # Analyser les pr√©dictions
        demands = [p['predicted_demand'] for p in predictions]
        max_demand = max(demands)
        avg_demand = np.mean(demands)

        # Stock actuel
        current_stock = forecast_result.get('contextual_insights', {}).get('current_stock', 0)

        # Recommandations bas√©es sur les vraies pr√©dictions
        if max_demand > avg_demand * 1.5:
            recommendations.append({
                'type': 'demand_spike',
                'priority': 'high',
                'message': f"Pic de demande pr√©vu: {max_demand} unit√©s. Pr√©voir un stock suppl√©mentaire.",
                'action': 'increase_collection'
            })

        if current_stock > 0:
            stock_duration = forecast_result.get('contextual_insights', {}).get('stock_days_remaining', 0)
            if stock_duration < 3:
                recommendations.append({
                    'type': 'low_stock',
                    'priority': 'critical',
                    'message': f"Stock critique: {current_stock} unit√©s pour {stock_duration} jours seulement.",
                    'action': 'urgent_collection'
                })
            elif stock_duration < 7:
                recommendations.append({
                    'type': 'moderate_stock',
                    'priority': 'medium',
                    'message': f"Stock mod√©r√©: planifier une collecte dans les prochains jours.",
                    'action': 'schedule_collection'
                })

        # Recommandations bas√©es sur la confiance du mod√®le
        avg_confidence = np.mean([p['confidence'] for p in predictions])
        if avg_confidence < 0.7:
            recommendations.append({
                'type': 'low_confidence',
                'priority': 'medium',
                'message': f"Confiance du mod√®le mod√©r√©e ({avg_confidence:.1%}). Surveiller de pr√®s les tendances r√©elles.",
                'action': 'monitor_closely'
            })

        return recommendations

    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©ration recommandations: {e}")
        return []


def get_available_methods():
    """
    üìã LISTE DES M√âTHODES DISPONIBLES pour l'interface
    """
    methods = [
        {
            'value': 'auto',
            'label': 'ü§ñ Auto-S√©lection',
            'description': 'S√©lection automatique de la meilleure m√©thode'
        },
        {
            'value': 'random_forest',
            'label': 'üå≤ Random Forest',
            'description': 'Apprentissage automatique robuste'
        }
    ]

    if XGBOOST_AVAILABLE:
        methods.append({
            'value': 'xgboost',
            'label': '‚ö° XGBoost',
            'description': 'Gradient boosting haute performance'
        })

    if STATSMODELS_AVAILABLE:
        methods.extend([
            {
                'value': 'arima',
                'label': 'üìà ARIMA',
                'description': 'Mod√®le statistique de s√©ries temporelles'
            },
            {
                'value': 'stl_arima',
                'label': 'üî¨ STL + ARIMA',
                'description': 'D√©composition saisonni√®re + ARIMA'
            }
        ])

    return methods


def health_check():
    """
    üè• V√âRIFICATION DE SANT√â DU SYST√àME
    """
    try:
        from django.db import connection

        # Test de connexion DB
        with connection.cursor() as cursor:
            cursor.execute("SELECT 1")
            db_status = "connected"

        return {
            'status': 'healthy',
            'version': '2.0-real-data',
            'database': db_status,
            'xgboost_available': XGBOOST_AVAILABLE,
            'statsmodels_available': STATSMODELS_AVAILABLE,
            'timestamp': datetime.now().isoformat()
        }

    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }


