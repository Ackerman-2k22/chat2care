# blood_demand_forecasting_improved.py - VERSION COMPL√àTEMENT AM√âLIOR√âE
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
import joblib
from datetime import datetime, timedelta
import warnings
import time
from django.core.cache import cache
from django.db.models import Q, Sum, Avg, Count
import logging
from datetime import datetime, timedelta
import json
from collections import defaultdict

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

# Imports conditionnels am√©lior√©s avec d√©tection renforc√©e
try:
    import xgboost as xgb

    XGBOOST_AVAILABLE = True
    logger.info("‚úÖ XGBoost available")
except ImportError:
    XGBOOST_AVAILABLE = False
    logger.info("‚ùå XGBoost not available")

try:
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.seasonal import STL, seasonal_decompose
    from statsmodels.tsa.stattools import adfuller
    from statsmodels.tsa.holtwinters import ExponentialSmoothing

    STATSMODELS_AVAILABLE = True
    logger.info("‚úÖ Statsmodels available")
except ImportError:
    STATSMODELS_AVAILABLE = False
    logger.info("‚ùå Statsmodels not available")

try:
    from lightgbm import LGBMRegressor

    LIGHTGBM_AVAILABLE = True
    logger.info("‚úÖ LightGBM available")
except ImportError:
    LIGHTGBM_AVAILABLE = False
    logger.info("‚ùå LightGBM not available")

# Imports des mod√®les Django am√©lior√©s
try:
    from .models import BloodUnit, BloodConsumption, BloodRequest, Donor

    MODELS_AVAILABLE = True
    logger.info("‚úÖ Django models imported successfully")
except ImportError:
    try:
        from app.models import BloodUnit, BloodConsumption, BloodRequest, Donor

        MODELS_AVAILABLE = True
        logger.info("‚úÖ Django models imported successfully (alternative path)")
    except ImportError:
        MODELS_AVAILABLE = False
        logger.warning("‚ö†Ô∏è Django models not available - using synthetic data")


class ImprovedBloodDemandForecaster:
    """
    üèÜ FORECASTER AM√âLIOR√â avec toutes les m√©thodes et haute pr√©cision
    """

    def __init__(self, max_execution_time=180):
        self.max_execution_time = max_execution_time
        self.start_time = None

        # Configuration √©tendue des mod√®les ML
        self.models = {
            'random_forest': RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=1
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            ),
            'linear_regression': Ridge(alpha=1.0),
            'linear_base': LinearRegression()
        }

        # Ajouter XGBoost si disponible
        if XGBOOST_AVAILABLE:
            self.models['xgboost'] = xgb.XGBRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                n_jobs=1,
                verbosity=0
            )
            logger.info("‚úÖ XGBoost model configured")

        # Ajouter LightGBM si disponible
        if LIGHTGBM_AVAILABLE:
            self.models['lightgbm'] = LGBMRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                verbosity=-1
            )
            logger.info("‚úÖ LightGBM model configured")

        self.scaler = StandardScaler()
        self.minmax_scaler = MinMaxScaler()
        self.trained_models = {}
        self.model_performance = {}
        self.arima_models = {}
        self.feature_importance = {}

        # Configuration enrichie des groupes sanguins
        self.blood_type_config = {
            'O+': {
                'priority': 'critical',
                'typical_weekend_factor': 0.7,
                'base_demand': 15,
                'seasonality_strength': 0.3,
                'volatility': 0.25,
                'compatibility': ['O+', 'A+', 'B+', 'AB+']
            },
            'A+': {
                'priority': 'high',
                'typical_weekend_factor': 0.75,
                'base_demand': 12,
                'seasonality_strength': 0.25,
                'volatility': 0.2,
                'compatibility': ['A+', 'AB+']
            },
            'B+': {
                'priority': 'medium',
                'typical_weekend_factor': 0.8,
                'base_demand': 8,
                'seasonality_strength': 0.2,
                'volatility': 0.22,
                'compatibility': ['B+', 'AB+']
            },
            'AB+': {
                'priority': 'low',
                'typical_weekend_factor': 0.85,
                'base_demand': 4,
                'seasonality_strength': 0.15,
                'volatility': 0.3,
                'compatibility': ['AB+']
            },
            'O-': {
                'priority': 'critical',
                'typical_weekend_factor': 0.6,
                'base_demand': 8,
                'seasonality_strength': 0.35,
                'volatility': 0.3,
                'compatibility': ['O+', 'O-', 'A+', 'A-', 'B+', 'B-', 'AB+', 'AB-']
            },
            'A-': {
                'priority': 'high',
                'typical_weekend_factor': 0.7,
                'base_demand': 6,
                'seasonality_strength': 0.25,
                'volatility': 0.25,
                'compatibility': ['A+', 'A-', 'AB+', 'AB-']
            },
            'B-': {
                'priority': 'medium',
                'typical_weekend_factor': 0.75,
                'base_demand': 4,
                'seasonality_strength': 0.2,
                'volatility': 0.28,
                'compatibility': ['B+', 'B-', 'AB+', 'AB-']
            },
            'AB-': {
                'priority': 'critical',
                'typical_weekend_factor': 0.8,
                'base_demand': 2,
                'seasonality_strength': 0.15,
                'volatility': 0.35,
                'compatibility': ['AB+', 'AB-']
            }
        }

    def check_timeout(self):
        """V√©rifier si on approche du timeout"""
        if self.start_time and time.time() - self.start_time > self.max_execution_time:
            raise TimeoutException("Maximum execution time exceeded")

    def get_enhanced_historical_data(self, blood_type, days_back=365):
        """
        üìä R√âCUP√âRATION ENRICHIE DES DONN√âES HISTORIQUES
        """
        if not MODELS_AVAILABLE:
            logger.warning(f"‚ùå Models not available for {blood_type}")
            return self.generate_enhanced_synthetic_data(blood_type, days_back)

        try:
            end_date = datetime.now().date()
            start_date = end_date - timedelta(days=days_back)

            logger.info(f"üìä R√©cup√©ration donn√©es enrichies pour {blood_type} ({start_date} √† {end_date})")

            # M√©thode 1: BloodConsumption avec jointures
            consumption_data = self._get_consumption_data(blood_type, start_date, end_date)

            # M√©thode 2: BloodRequest avec statuts
            request_data = self._get_request_data(blood_type, start_date, end_date)

            # M√©thode 3: BloodUnit usage tracking
            usage_data = self._get_usage_data(blood_type, start_date, end_date)

            # Combiner les sources de donn√©es
            combined_data = self._combine_data_sources(
                consumption_data, request_data, usage_data, start_date, end_date, blood_type
            )

            if combined_data is not None and len(combined_data) > 7:
                logger.info(f"‚úÖ Donn√©es r√©elles r√©cup√©r√©es: {len(combined_data)} jours")
                return combined_data
            else:
                logger.warning(f"‚ö†Ô∏è Donn√©es insuffisantes, g√©n√©ration synth√©tique am√©lior√©e")
                return self.generate_enhanced_synthetic_data(blood_type, days_back)

        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration donn√©es: {e}")
            return self.generate_enhanced_synthetic_data(blood_type, days_back)

    def _get_consumption_data(self, blood_type, start_date, end_date):
        """R√©cup√©rer les donn√©es de consommation"""
        try:
            daily_consumption = BloodConsumption.objects.filter(
                unit__donor__blood_type=blood_type,
                date__range=[start_date, end_date]
            ).extra(
                select={'day': 'DATE(date)'}
            ).values('day').annotate(
                total_volume=Sum('volume'),
                unit_count=Count('unit_id')
            ).order_by('day')

            return list(daily_consumption)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Consumption data failed: {e}")
            return []

    def _get_request_data(self, blood_type, start_date, end_date):
        """R√©cup√©rer les donn√©es de demandes"""
        try:
            daily_requests = BloodRequest.objects.filter(
                blood_type=blood_type,
                request_date__range=[start_date, end_date],
                status__in=['Fulfilled', 'Approved', 'Completed']
            ).extra(
                select={'day': 'DATE(request_date)'}
            ).values('day').annotate(
                total_quantity=Sum('quantity'),
                urgent_count=Count('request_id', filter=Q(priority='Urgent')),
                request_count=Count('request_id')
            ).order_by('day')

            return list(daily_requests)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Request data failed: {e}")
            return []

    def _get_usage_data(self, blood_type, start_date, end_date):
        """R√©cup√©rer les donn√©es d'utilisation"""
        try:
            daily_usage = BloodUnit.objects.filter(
                donor__blood_type=blood_type,
                status='Used',
                collection_date__range=[start_date, end_date]
            ).extra(
                select={'day': 'DATE(collection_date)'}
            ).values('day').annotate(
                total_units=Count('unit_id'),
                total_volume=Sum('volume_ml')
            ).order_by('day')

            return list(daily_usage)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Usage data failed: {e}")
            return []

    def _combine_data_sources(self, consumption_data, request_data, usage_data, start_date, end_date, blood_type):
        """
        üîÑ COMBINAISON INTELLIGENTE DES SOURCES DE DONN√âES
        """
        try:
            # Cr√©er un dictionnaire pour combiner toutes les sources
            combined_dict = defaultdict(lambda: {
                'demand': 0,
                'volume': 0,
                'requests': 0,
                'urgent': 0,
                'confidence': 0.5
            })

            # Poids pour chaque source de donn√©es
            weights = {'consumption': 0.4, 'requests': 0.4, 'usage': 0.2}

            # Traiter les donn√©es de consommation
            for record in consumption_data:
                day = record['day']
                volume = record.get('total_volume', 0) or 0
                units = record.get('unit_count', 0) or 0

                # Convertir volume en unit√©s si n√©cessaire
                if volume > 100:
                    demand = max(1, int(volume / 450))  # 450ml par unit√©
                else:
                    demand = max(units, 1)

                combined_dict[day]['demand'] += demand * weights['consumption']
                combined_dict[day]['volume'] += volume
                combined_dict[day]['confidence'] += 0.3

            # Traiter les donn√©es de demandes
            for record in request_data:
                day = record['day']
                quantity = record.get('total_quantity', 0) or 0
                urgent = record.get('urgent_count', 0) or 0

                combined_dict[day]['demand'] += quantity * weights['requests']
                combined_dict[day]['requests'] += quantity
                combined_dict[day]['urgent'] += urgent
                combined_dict[day]['confidence'] += 0.3

            # Traiter les donn√©es d'utilisation
            for record in usage_data:
                day = record['day']
                units = record.get('total_units', 0) or 0
                volume = record.get('total_volume', 0) or 0

                if volume > 100:
                    demand = max(1, int(volume / 450))
                else:
                    demand = units

                combined_dict[day]['demand'] += demand * weights['usage']
                combined_dict[day]['confidence'] += 0.2

            # Convertir en DataFrame
            df_data = []
            date_range = pd.date_range(start_date, end_date, freq='D')

            config = self.blood_type_config.get(blood_type, {})
            base_demand = config.get('base_demand', 5)

            for date in date_range:
                day_str = date.date()
                data = combined_dict.get(day_str, {})

                demand = data.get('demand', 0)
                confidence = min(1.0, data.get('confidence', 0.1))

                # Si pas de donn√©es, utiliser des estimations bas√©es sur les patterns
                if demand == 0:
                    demand = self._estimate_missing_demand(date, combined_dict, base_demand)
                    confidence = max(0.1, confidence)

                # Lissage et validation
                demand = max(0, int(demand))

                df_data.append({
                    'date': date,
                    'demand': demand,
                    'volume': data.get('volume', demand * 450),
                    'requests': data.get('requests', 0),
                    'urgent': data.get('urgent', 0),
                    'confidence': confidence,
                    'data_quality': 'real' if confidence > 0.2 else 'estimated'
                })

            df = pd.DataFrame(df_data)
            df = df.set_index('date')

            # Post-traitement pour am√©liorer la qualit√©
            df = self._post_process_data(df, blood_type)

            logger.info(f"‚úÖ Donn√©es combin√©es: {len(df)} jours, demande moyenne: {df['demand'].mean():.1f}")
            return df

        except Exception as e:
            logger.error(f"‚ùå Erreur combinaison donn√©es: {e}")
            return None

    def _estimate_missing_demand(self, date, combined_dict, base_demand):
        """Estimer la demande pour les jours manquants"""
        try:
            # Chercher les donn√©es des jours voisins
            nearby_demands = []
            for i in range(-3, 4):
                if i == 0:
                    continue
                nearby_date = (date + timedelta(days=i)).date()
                if nearby_date in combined_dict:
                    nearby_demands.append(combined_dict[nearby_date]['demand'])

            if nearby_demands:
                return np.mean(nearby_demands)
            else:
                # Pattern hebdomadaire basique
                weekend_factor = 0.7 if date.weekday() in [5, 6] else 1.0
                return base_demand * weekend_factor

        except:
            return base_demand

    def _post_process_data(self, df, blood_type):
        """Post-traitement pour am√©liorer la qualit√© des donn√©es"""
        try:
            # Lissage des valeurs aberrantes
            df['demand_smoothed'] = df['demand'].rolling(
                window=3, min_periods=1, center=True
            ).mean()

            # D√©tection des outliers
            Q1 = df['demand'].quantile(0.25)
            Q3 = df['demand'].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            # Remplacer les outliers par des valeurs liss√©es
            outlier_mask = (df['demand'] < lower_bound) | (df['demand'] > upper_bound)
            df.loc[outlier_mask, 'demand'] = df.loc[outlier_mask, 'demand_smoothed']

            # Assurer la coh√©rence
            df['demand'] = df['demand'].round().astype(int)
            df['demand'] = df['demand'].clip(lower=0)

            # Ajouter des features temporelles enrichies
            df['day_of_week'] = df.index.dayofweek
            df['month'] = df.index.month
            df['quarter'] = df.index.quarter
            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
            df['is_holiday'] = self._detect_holidays(df.index)

            return df.drop('demand_smoothed', axis=1)

        except Exception as e:
            logger.error(f"‚ùå Erreur post-traitement: {e}")
            return df

    def _detect_holidays(self, date_index):
        """D√©tection simple des jours f√©ri√©s"""
        try:
            holidays = []
            for date in date_index:
                # Jours f√©ri√©s fran√ßais principaux
                is_holiday = (
                        (date.month == 1 and date.day == 1) or  # Nouvel an
                        (date.month == 5 and date.day == 1) or  # F√™te du travail
                        (date.month == 7 and date.day == 14) or  # F√™te nationale
                        (date.month == 8 and date.day == 15) or  # Assomption
                        (date.month == 11 and date.day == 1) or  # Toussaint
                        (date.month == 11 and date.day == 11) or  # Armistice
                        (date.month == 12 and date.day == 25)  # No√´l
                )
                holidays.append(int(is_holiday))
            return holidays
        except:
            return [0] * len(date_index)

    def generate_enhanced_synthetic_data(self, blood_type, days_back):
        """
        üè≠ G√âN√âRATION DE DONN√âES SYNTH√âTIQUES AM√âLIOR√âES
        """
        try:
            logger.info(f"üè≠ G√©n√©ration donn√©es synth√©tiques am√©lior√©es pour {blood_type}")

            end_date = datetime.now().date()
            start_date = end_date - timedelta(days=days_back)

            config = self.blood_type_config.get(blood_type, {})
            base_demand = config.get('base_demand', 5)
            weekend_factor = config.get('typical_weekend_factor', 0.8)
            seasonality_strength = config.get('seasonality_strength', 0.2)
            volatility = config.get('volatility', 0.2)

            date_range = pd.date_range(start_date, end_date, freq='D')
            synthetic_data = []

            # Composantes du mod√®le synth√©tique am√©lior√©
            for i, date in enumerate(date_range):
                # 1. Tendance l√©g√®re
                trend = 1 + (i / len(date_range)) * 0.15

                # 2. Saisonnalit√© annuelle
                day_of_year = date.timetuple().tm_yday
                seasonal_annual = 1 + seasonality_strength * np.sin(2 * np.pi * (day_of_year - 80) / 365)

                # 3. Saisonnalit√© hebdomadaire
                weekday_factor = weekend_factor if date.weekday() in [5, 6] else 1.0
                if date.weekday() == 0:  # Lundi
                    weekday_factor *= 1.2  # Plus de demande le lundi

                # 4. Effets sp√©ciaux (vacances, √©v√©nements)
                special_factor = 1.0
                if date.month in [7, 8]:  # √ât√©
                    special_factor *= 0.85
                elif date.month in [12, 1]:  # Hiver
                    special_factor *= 1.15

                # 5. Bruit r√©aliste avec autocorr√©lation
                if i == 0:
                    noise = np.random.normal(0, volatility)
                else:
                    # Autocorr√©lation simple
                    prev_noise = synthetic_data[i - 1].get('noise', 0)
                    noise = 0.3 * prev_noise + 0.7 * np.random.normal(0, volatility)

                # 6. √âv√©nements rares (pics de demande)
                event_factor = 1.0
                if np.random.random() < 0.03:  # 3% de chance d'√©v√©nement
                    event_factor = np.random.uniform(1.5, 2.5)

                # Calcul final
                demand = base_demand * trend * seasonal_annual * weekday_factor * special_factor * event_factor * (
                            1 + noise)
                demand = max(0, int(demand))

                synthetic_data.append({
                    'date': date,
                    'demand': demand,
                    'volume': demand * 450,
                    'requests': max(0, demand + np.random.randint(-2, 3)),
                    'urgent': max(0, int(demand * 0.1 + np.random.randint(0, 2))),
                    'confidence': 0.7,  # Confiance √©lev√©e pour les donn√©es synth√©tiques
                    'data_quality': 'synthetic_enhanced',
                    'noise': noise
                })

            df = pd.DataFrame(synthetic_data)
            df = df.set_index('date')

            # Ajouter les features temporelles
            df['day_of_week'] = df.index.dayofweek
            df['month'] = df.index.month
            df['quarter'] = df.index.quarter
            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
            df['is_holiday'] = self._detect_holidays(df.index)

            logger.info(f"‚úÖ Donn√©es synth√©tiques am√©lior√©es: {len(df)} jours, moyenne: {df['demand'].mean():.1f}")
            return df

        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration synth√©tique: {e}")
            # Fallback simple
            return self._generate_simple_fallback_data(blood_type, days_back)

    def _generate_simple_fallback_data(self, blood_type, days_back):
        """G√©n√©ration de donn√©es de secours ultra-simple"""
        try:
            config = self.blood_type_config.get(blood_type, {})
            base = config.get('base_demand', 5)

            simple_data = []
            for i in range(days_back):
                date = datetime.now().date() - timedelta(days=i)
                demand = base + np.random.randint(-2, 3)
                simple_data.append({
                    'date': date,
                    'demand': max(1, demand),
                    'confidence': 0.5,
                    'data_quality': 'fallback'
                })

            return pd.DataFrame(simple_data).set_index('date').sort_index()
        except:
            return None

    def prepare_advanced_features(self, df, contextual_data=None):
        """
        üõ†Ô∏è PR√âPARATION DE FEATURES AVANC√âES
        """
        if df is None or len(df) < 7:
            return None

        df = df.copy()

        try:
            # Features temporelles avanc√©es
            df['day_of_week'] = df.index.dayofweek
            df['month'] = df.index.month
            df['day_of_month'] = df.index.day
            df['quarter'] = df.index.quarter
            df['week_of_year'] = df.index.isocalendar().week

            # Features bool√©ennes
            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
            df['is_monday'] = (df['day_of_week'] == 0).astype(int)
            df['is_friday'] = (df['day_of_week'] == 4).astype(int)
            df['is_month_start'] = (df['day_of_month'] <= 5).astype(int)
            df['is_month_end'] = (df['day_of_month'] >= 25).astype(int)

            # Moyennes mobiles multiples
            for window in [3, 7, 14, 30]:
                if len(df) >= window:
                    df[f'demand_ma_{window}'] = df['demand'].rolling(
                        window=window, min_periods=max(1, window // 2)
                    ).mean()
                    df[f'demand_std_{window}'] = df['demand'].rolling(
                        window=window, min_periods=max(1, window // 2)
                    ).std().fillna(0)
                else:
                    df[f'demand_ma_{window}'] = df['demand'].mean()
                    df[f'demand_std_{window}'] = df['demand'].std()

            # Moyennes mobiles exponentielles
            for alpha in [0.1, 0.3, 0.5]:
                df[f'demand_ema_{alpha}'] = df['demand'].ewm(alpha=alpha).mean()

            # Lags multiples
            for lag in [1, 2, 3, 7, 14]:
                if len(df) > lag:
                    df[f'demand_lag_{lag}'] = df['demand'].shift(lag)
                else:
                    df[f'demand_lag_{lag}'] = df['demand'].mean()

            # Features de tendances
            for window in [7, 14, 30]:
                if len(df) >= window:
                    df[f'demand_trend_{window}'] = df['demand'].rolling(
                        window, min_periods=max(3, window // 3)
                    ).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 3 else 0, raw=False)
                else:
                    df[f'demand_trend_{window}'] = 0

            # Features de volatilit√©
            for window in [7, 14]:
                if len(df) >= window:
                    df[f'demand_volatility_{window}'] = df['demand'].rolling(
                        window, min_periods=max(3, window // 2)
                    ).std()
                else:
                    df[f'demand_volatility_{window}'] = df['demand'].std()

            # Features cycliques (encodage trigonom√©trique)
            df['sin_day_of_week'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
            df['cos_day_of_week'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
            df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)
            df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)
            df['sin_day_of_month'] = np.sin(2 * np.pi * df['day_of_month'] / 31)
            df['cos_day_of_month'] = np.cos(2 * np.pi * df['day_of_month'] / 31)

            # Features d'interaction
            df['weekend_month'] = df['is_weekend'] * df['month']
            df['monday_quarter'] = df['is_monday'] * df['quarter']

            # Features d√©riv√©es de la demande
            df['demand_diff'] = df['demand'].diff().fillna(0)
            df['demand_pct_change'] = df['demand'].pct_change().fillna(0)

            # Features bas√©es sur les quantiles
            if len(df) >= 30:
                rolling_quantiles = df['demand'].rolling(30, min_periods=10)
                df['demand_quantile_25'] = rolling_quantiles.quantile(0.25)
                df['demand_quantile_75'] = rolling_quantiles.quantile(0.75)
                df['demand_above_q75'] = (df['demand'] > df['demand_quantile_75']).astype(int)
                df['demand_below_q25'] = (df['demand'] < df['demand_quantile_25']).astype(int)
            else:
                q25, q75 = df['demand'].quantile([0.25, 0.75])
                df['demand_quantile_25'] = q25
                df['demand_quantile_75'] = q75
                df['demand_above_q75'] = (df['demand'] > q75).astype(int)
                df['demand_below_q25'] = (df['demand'] < q25).astype(int)

            # Features contextuelles enrichies
            if contextual_data:
                mean_demand = df['demand'].mean()
                df['stock_ratio'] = contextual_data.get('current_stock', 0) / max(1, mean_demand)
                df['recent_trend_factor'] = contextual_data.get('recent_daily_avg', 0) / max(1, mean_demand)
                df['urgent_factor'] = contextual_data.get('urgent_requests', 0) / max(1, mean_demand)
                df['expiry_pressure'] = max(0, 30 - contextual_data.get('avg_expiry_days', 30)) / 30
            else:
                df['stock_ratio'] = 1.0
                df['recent_trend_factor'] = 1.0
                df['urgent_factor'] = 0.0
                df['expiry_pressure'] = 0.0

            # Features de saisonnalit√© avanc√©es
            df['season'] = df['month'] % 12 // 3  # 0=hiver, 1=printemps, 2=√©t√©, 3=automne
            df['is_summer'] = df['season'].eq(2).astype(int)
            df['is_winter'] = df['season'].eq(0).astype(int)

            # Features de patterns complexes
            if 'requests' in df.columns:
                df['demand_request_ratio'] = df['demand'] / (df['requests'] + 1)
            if 'urgent' in df.columns:
                df['urgent_ratio'] = df['urgent'] / (df['demand'] + 1)

            # Nettoyage final
            df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)

            # Remplacement des valeurs infinies
            df = df.replace([np.inf, -np.inf], 0)

            logger.info(f"‚úÖ Features avanc√©es cr√©√©es: {len(df.select_dtypes(include=[np.number]).columns)} features")
            return df

        except Exception as e:
            logger.error(f"‚ùå Erreur feature engineering avanc√©: {e}")
            return None

    def train_all_models_improved(self, blood_type, method='auto'):
        """
        üéØ ENTRA√éNEMENT DE TOUS LES MOD√àLES AM√âLIOR√â
        """
        self.start_time = time.time()

        cache_key = f'improved_model_{blood_type}_{method}'
        cached_result = cache.get(cache_key)
        if cached_result:
            logger.info(f"‚úÖ Mod√®les en cache pour {blood_type}")
            self.model_performance[blood_type] = cached_result['performance']
            self.trained_models.update(cached_result['models'])
            return cached_result['performance'], cached_result['best_method']

        try:
            # R√©cup√©ration des donn√©es enrichies
            historical_data = self.get_enhanced_historical_data(blood_type, days_back=365)
            if historical_data is None or len(historical_data) < 14:
                logger.warning(f"‚ö†Ô∏è Donn√©es insuffisantes pour {blood_type}")
                return self._create_enhanced_fallback_model(blood_type)

            contextual_data = self.get_enhanced_contextual_data(blood_type)

            logger.info(f"üî¨ Entra√Ænement mod√®les am√©lior√©s pour {blood_type} avec {len(historical_data)} jours")

            # Pr√©paration des features avanc√©es
            df_features = self.prepare_advanced_features(historical_data, contextual_data)
            if df_features is None or len(df_features) < 10:
                return self._create_enhanced_fallback_model(blood_type)

            # S√©lection intelligente des features
            feature_cols = self._select_best_features(df_features)

            X = df_features[feature_cols]
            y = df_features['demand']

            # Split temporel am√©lior√©
            tscv = TimeSeriesSplit(n_splits=3)
            results = {}

            # Entra√Æner tous les mod√®les disponibles
            model_results = {}

            # 1. Random Forest
            model_results['random_forest'] = self._train_and_evaluate_model(
                'random_forest', X, y, tscv, feature_cols, blood_type
            )

            # 2. Gradient Boosting
            model_results['gradient_boosting'] = self._train_and_evaluate_model(
                'gradient_boosting', X, y, tscv, feature_cols, blood_type
            )

            # 3. Linear Models
            model_results['linear_regression'] = self._train_and_evaluate_model(
                'linear_regression', X, y, tscv, feature_cols, blood_type
            )

            # 4. XGBoost si disponible
            if XGBOOST_AVAILABLE:
                model_results['xgboost'] = self._train_and_evaluate_model(
                    'xgboost', X, y, tscv, feature_cols, blood_type
                )

            # 5. LightGBM si disponible
            if LIGHTGBM_AVAILABLE:
                model_results['lightgbm'] = self._train_and_evaluate_model(
                    'lightgbm', X, y, tscv, feature_cols, blood_type
                )

            # 6. ARIMA si disponible et appropri√©
            if STATSMODELS_AVAILABLE and len(historical_data) >= 30:
                model_results['arima'] = self._train_arima_model(historical_data, blood_type)
                model_results['stl_arima'] = self._train_stl_arima_model(historical_data, blood_type)
                model_results['exponential_smoothing'] = self._train_exponential_smoothing(historical_data, blood_type)

            # 7. Ensemble Methods
            if len(model_results) >= 2:
                model_results['ensemble'] = self._create_ensemble_model(model_results, X, y, feature_cols, blood_type)

            # Filtrer les r√©sultats valides
            valid_results = {k: v for k, v in model_results.items() if v is not None}

            if not valid_results:
                return self._create_enhanced_fallback_model(blood_type)

            # S√©lection du meilleur mod√®le bas√© sur plusieurs crit√®res
            best_method = self._select_best_model(valid_results)

            # Cache des r√©sultats avec dur√©e adaptative
            cache_duration = self._calculate_cache_duration(valid_results[best_method])
            cache_data = {
                'performance': valid_results,
                'models': {k: v for k, v in self.trained_models.items() if blood_type in k},
                'best_method': best_method,
                'feature_importance': self.feature_importance.get(blood_type, {}),
                'data_points': len(historical_data),
                'contextual_data': contextual_data
            }
            cache.set(cache_key, cache_data, cache_duration)

            self.model_performance[blood_type] = valid_results

            logger.info(f"‚úÖ Mod√®les entra√Æn√©s: {best_method} s√©lectionn√© "
                        f"(MAPE: {valid_results[best_method].get('mape', 0):.2f}%)")

            return valid_results, best_method

        except Exception as e:
            logger.error(f"‚ùå Erreur entra√Ænement mod√®les: {e}")
            return self._create_enhanced_fallback_model(blood_type)

    def _select_best_features(self, df):
        """S√©lection intelligente des meilleures features"""
        try:
            # Exclure les colonnes non-features
            exclude_cols = ['demand', 'volume', 'requests', 'urgent', 'confidence', 'data_quality', 'noise']
            feature_cols = [col for col in df.columns if col not in exclude_cols]

            # Calculer la corr√©lation avec la target
            correlations = df[feature_cols].corrwith(df['demand']).abs()

            # S√©lectionner les features les plus corr√©l√©es (mais pas trop pour √©viter l'overfitting)
            selected_features = correlations.nlargest(min(50, len(feature_cols))).index.tolist()

            # Toujours inclure les features de base importantes
            essential_features = [
                'day_of_week', 'month', 'is_weekend', 'demand_lag_1', 'demand_lag_7',
                'demand_ma_7', 'demand_ma_14', 'sin_day_of_week', 'cos_day_of_week'
            ]

            for feature in essential_features:
                if feature in feature_cols and feature not in selected_features:
                    selected_features.append(feature)

            return selected_features[:60]  # Limiter √† 60 features max

        except Exception as e:
            logger.error(f"‚ùå Erreur s√©lection features: {e}")
            return [col for col in df.columns if col not in ['demand', 'volume', 'requests', 'urgent']]

    def _train_and_evaluate_model(self, model_name, X, y, tscv, feature_cols, blood_type):
        """Entra√Æner et √©valuer un mod√®le ML"""
        try:
            model = self.models[model_name]
            scores = {'mae': [], 'rmse': [], 'mape': []}

            # Validation crois√©e temporelle
            for train_idx, test_idx in tscv.split(X):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

                # Entra√Ænement
                if model_name in ['linear_regression', 'linear_base']:
                    # Normalisation pour les mod√®les lin√©aires
                    scaler = StandardScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    model.fit(X_train_scaled, y_train)
                    y_pred = model.predict(X_test_scaled)
                else:
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)

                # √âvaluation
                y_pred = np.maximum(y_pred, 0)  # Pas de pr√©dictions n√©gatives

                mae = mean_absolute_error(y_test, y_pred)
                rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                mape = mean_absolute_percentage_error(y_test, y_pred) * 100

                scores['mae'].append(mae)
                scores['rmse'].append(rmse)
                scores['mape'].append(min(mape, 100))  # Cap MAPE √† 100%

            # Entra√Ænement final sur toutes les donn√©es
            if model_name in ['linear_regression', 'linear_base']:
                final_scaler = StandardScaler()
                X_scaled = final_scaler.fit_transform(X)
                model.fit(X_scaled, y)
                self.trained_models[f'{model_name}_{blood_type}'] = {
                    'model': model,
                    'features': feature_cols,
                    'scaler': final_scaler,
                    'trained_date': datetime.now()
                }
            else:
                model.fit(X, y)
                self.trained_models[f'{model_name}_{blood_type}'] = {
                    'model': model,
                    'features': feature_cols,
                    'scaler': None,
                    'trained_date': datetime.now()
                }

            # Feature importance si disponible
            if hasattr(model, 'feature_importances_'):
                importance_dict = dict(zip(feature_cols, model.feature_importances_))
                if blood_type not in self.feature_importance:
                    self.feature_importance[blood_type] = {}
                self.feature_importance[blood_type][model_name] = importance_dict

            # Calcul des m√©triques moyennes
            result = {
                'mae': np.mean(scores['mae']),
                'rmse': np.mean(scores['rmse']),
                'mape': np.mean(scores['mape']),
                'mae_std': np.std(scores['mae']),
                'rmse_std': np.std(scores['rmse']),
                'mape_std': np.std(scores['mape']),
                'training_samples': len(X),
                'cv_folds': len(scores['mae']),
                'model_type': 'machine_learning'
            }

            logger.info(f"‚úÖ {model_name} entra√Æn√©: MAPE {result['mape']:.2f}¬±{result['mape_std']:.2f}%")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur entra√Ænement {model_name}: {e}")
            return None

    def _train_arima_model(self, historical_data, blood_type):
        """Entra√Æner un mod√®le ARIMA"""
        try:
            if not STATSMODELS_AVAILABLE:
                return None

            ts_data = historical_data['demand'].dropna()
            if len(ts_data) < 20:
                return None

            # Test de stationnarit√©
            adf_result = adfuller(ts_data)
            is_stationary = adf_result[1] < 0.05

            # Param√®tres ARIMA optimaux (recherche simple)
            best_aic = float('inf')
            best_params = (1, 1, 1)

            for p in range(0, 3):
                for d in range(0, 2 if not is_stationary else 1):
                    for q in range(0, 3):
                        try:
                            model = ARIMA(ts_data, order=(p, d, q))
                            fitted_model = model.fit()
                            if fitted_model.aic < best_aic:
                                best_aic = fitted_model.aic
                                best_params = (p, d, q)
                        except:
                            continue

            # Entra√Ænement du meilleur mod√®le
            final_model = ARIMA(ts_data, order=best_params)
            fitted_final = final_model.fit()

            # √âvaluation avec validation crois√©e temporelle
            n_test = min(7, len(ts_data) // 4)
            train_data = ts_data[:-n_test]
            test_data = ts_data[-n_test:]

            test_model = ARIMA(train_data, order=best_params)
            test_fitted = test_model.fit()
            forecast = test_fitted.forecast(steps=n_test)

            mae = mean_absolute_error(test_data, forecast)
            rmse = np.sqrt(mean_squared_error(test_data, forecast))
            mape = mean_absolute_percentage_error(test_data, forecast) * 100

            # Stocker le mod√®le
            self.arima_models[f'arima_{blood_type}'] = {
                'model': fitted_final,
                'order': best_params,
                'aic': best_aic,
                'trained_date': datetime.now()
            }

            result = {
                'mae': mae,
                'rmse': rmse,
                'mape': min(mape, 100),
                'aic': best_aic,
                'order': best_params,
                'training_samples': len(ts_data),
                'model_type': 'time_series'
            }

            logger.info(f"‚úÖ ARIMA{best_params} entra√Æn√©: MAPE {result['mape']:.2f}%")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur ARIMA: {e}")
            return None

    def _train_stl_arima_model(self, historical_data, blood_type):
        """Entra√Æner un mod√®le STL + ARIMA"""
        try:
            if not STATSMODELS_AVAILABLE:
                return None

            ts_data = historical_data['demand'].dropna()
            if len(ts_data) < 30:
                return None

            # D√©composition STL
            stl = STL(ts_data, seasonal=7 if len(ts_data) >= 14 else None)
            stl_result = stl.fit()

            # ARIMA sur les r√©sidus
            residuals = stl_result.resid.dropna()

            # Param√®tres ARIMA pour les r√©sidus
            best_aic = float('inf')
            best_params = (1, 0, 1)

            for p in range(0, 3):
                for d in range(0, 2):
                    for q in range(0, 3):
                        try:
                            model = ARIMA(residuals, order=(p, d, q))
                            fitted_model = model.fit()
                            if fitted_model.aic < best_aic:
                                best_aic = fitted_model.aic
                                best_params = (p, d, q)
                        except:
                            continue

            # Mod√®le final
            final_arima = ARIMA(residuals, order=best_params)
            fitted_arima = final_arima.fit()

            # √âvaluation
            n_test = min(7, len(ts_data) // 4)
            train_data = ts_data[:-n_test]
            test_data = ts_data[-n_test:]

            # Pr√©diction STL+ARIMA simplifi√©e
            trend_forecast = stl_result.trend[-n_test:].values
            seasonal_forecast = stl_result.seasonal[-n_test:].values
            residual_forecast = fitted_arima.forecast(steps=n_test)

            forecast = trend_forecast + seasonal_forecast + residual_forecast
            forecast = np.maximum(forecast, 0)

            mae = mean_absolute_error(test_data, forecast)
            rmse = np.sqrt(mean_squared_error(test_data, forecast))
            mape = mean_absolute_percentage_error(test_data, forecast) * 100

            # Stocker le mod√®le
            self.arima_models[f'stl_arima_{blood_type}'] = {
                'stl_model': stl_result,
                'arima_model': fitted_arima,
                'order': best_params,
                'aic': best_aic,
                'trained_date': datetime.now()
            }

            result = {
                'mae': mae,
                'rmse': rmse,
                'mape': min(mape, 100),
                'aic': best_aic,
                'order': best_params,
                'training_samples': len(ts_data),
                'model_type': 'time_series_advanced'
            }

            logger.info(f"‚úÖ STL-ARIMA entra√Æn√©: MAPE {result['mape']:.2f}%")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur STL-ARIMA: {e}")
            return None

    def _train_exponential_smoothing(self, historical_data, blood_type):
        """Entra√Æner un mod√®le de lissage exponentiel"""
        try:
            if not STATSMODELS_AVAILABLE:
                return None

            ts_data = historical_data['demand'].dropna()
            if len(ts_data) < 20:
                return None

            # Essayer diff√©rentes configurations
            configs = [
                {'seasonal': None},
                {'seasonal': 'add', 'seasonal_periods': 7} if len(ts_data) >= 14 else {'seasonal': None},
                {'seasonal': 'mul', 'seasonal_periods': 7} if len(ts_data) >= 14 else {'seasonal': None}
            ]

            best_aic = float('inf')
            best_model = None
            best_config = None

            for config in configs:
                try:
                    model = ExponentialSmoothing(ts_data, **config)
                    fitted_model = model.fit()
                    if fitted_model.aic < best_aic:
                        best_aic = fitted_model.aic
                        best_model = fitted_model
                        best_config = config
                except:
                    continue

            if best_model is None:
                return None

            # √âvaluation
            n_test = min(7, len(ts_data) // 4)
            train_data = ts_data[:-n_test]
            test_data = ts_data[-n_test:]

            test_model = ExponentialSmoothing(train_data, **best_config)
            test_fitted = test_model.fit()
            forecast = test_fitted.forecast(steps=n_test)
            forecast = np.maximum(forecast, 0)

            mae = mean_absolute_error(test_data, forecast)
            rmse = np.sqrt(mean_squared_error(test_data, forecast))
            mape = mean_absolute_percentage_error(test_data, forecast) * 100

            # Stocker le mod√®le
            self.arima_models[f'exp_smooth_{blood_type}'] = {
                'model': best_model,
                'config': best_config,
                'aic': best_aic,
                'trained_date': datetime.now()
            }

            result = {
                'mae': mae,
                'rmse': rmse,
                'mape': min(mape, 100),
                'aic': best_aic,
                'config': best_config,
                'training_samples': len(ts_data),
                'model_type': 'exponential_smoothing'
            }

            logger.info(f"‚úÖ Exp Smoothing entra√Æn√©: MAPE {result['mape']:.2f}%")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur Exponential Smoothing: {e}")
            return None

    def _create_ensemble_model(self, model_results, X, y, feature_cols, blood_type):
        """Cr√©er un mod√®le d'ensemble"""
        try:
            # S√©lectionner les 3 meilleurs mod√®les ML
            ml_models = {k: v for k, v in model_results.items()
                         if v and v.get('model_type') == 'machine_learning'}

            if len(ml_models) < 2:
                return None

            # Trier par performance
            sorted_models = sorted(ml_models.items(),
                                   key=lambda x: x[1].get('mape', 100))[:3]

            # Calculer les poids bas√©s sur la performance inverse
            weights = []
            total_inverse_mape = 0

            for _, performance in sorted_models:
                mape = max(performance.get('mape', 50), 1)  # √âviter division par 0
                inverse_mape = 1 / mape
                weights.append(inverse_mape)
                total_inverse_mape += inverse_mape

            # Normaliser les poids
            weights = [w / total_inverse_mape for w in weights]

            # √âvaluation de l'ensemble avec validation crois√©e
            tscv = TimeSeriesSplit(n_splits=3)
            ensemble_scores = {'mae': [], 'rmse': [], 'mape': []}

            for train_idx, test_idx in tscv.split(X):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

                ensemble_pred = np.zeros(len(y_test))

                for i, (model_name, _) in enumerate(sorted_models):
                    model_key = f'{model_name}_{blood_type}'
                    if model_key in self.trained_models:
                        model_data = self.trained_models[model_key]
                        model = model_data['model']
                        scaler = model_data.get('scaler')

                        if scaler:
                            X_test_scaled = scaler.transform(X_test)
                            pred = model.predict(X_test_scaled)
                        else:
                            pred = model.predict(X_test)

                        pred = np.maximum(pred, 0)
                        ensemble_pred += weights[i] * pred

                mae = mean_absolute_error(y_test, ensemble_pred)
                rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))
                mape = mean_absolute_percentage_error(y_test, ensemble_pred) * 100

                ensemble_scores['mae'].append(mae)
                ensemble_scores['rmse'].append(rmse)
                ensemble_scores['mape'].append(min(mape, 100))

            # Stocker les informations de l'ensemble
            self.trained_models[f'ensemble_{blood_type}'] = {
                'models': [name for name, _ in sorted_models],
                'weights': weights,
                'features': feature_cols,
                'trained_date': datetime.now()
            }

            result = {
                'mae': np.mean(ensemble_scores['mae']),
                'rmse': np.mean(ensemble_scores['rmse']),
                'mape': np.mean(ensemble_scores['mape']),
                'mae_std': np.std(ensemble_scores['mae']),
                'rmse_std': np.std(ensemble_scores['rmse']),
                'mape_std': np.std(ensemble_scores['mape']),
                'models_used': [name for name, _ in sorted_models],
                'weights': weights,
                'training_samples': len(X),
                'model_type': 'ensemble'
            }

            logger.info(f"‚úÖ Ensemble cr√©√©: MAPE {result['mape']:.2f}%")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation ensemble: {e}")
            return None

    def _select_best_model(self, model_results):
        """S√©lectionner le meilleur mod√®le bas√© sur plusieurs crit√®res"""
        try:
            if not model_results:
                return 'fallback'

            # Crit√®res de s√©lection pond√©r√©s
            scores = {}

            for model_name, performance in model_results.items():
                if not performance:
                    continue

                # MAPE (40% du score)
                mape = performance.get('mape', 100)
                mape_score = max(0, (100 - mape) / 100)

                # Stabilit√© (20% du score) - bas√© sur std si disponible
                stability_score = 1.0
                if 'mape_std' in performance:
                    mape_std = performance.get('mape_std', 0)
                    stability_score = max(0, 1 - (mape_std / 20))  # P√©naliser forte variabilit√©

                # Complexit√©/Robustesse (20% du score)
                complexity_score = 1.0
                if model_name in ['ensemble']:
                    complexity_score = 1.2  # Bonus pour ensemble
                elif model_name in ['xgboost', 'lightgbm']:
                    complexity_score = 1.1  # Bonus pour mod√®les avanc√©s
                elif model_name in ['arima', 'stl_arima']:
                    complexity_score = 1.05  # L√©ger bonus pour time series

                # √âchantillons d'entra√Ænement (20% du score)
                training_samples = performance.get('training_samples', 10)
                sample_score = min(1.0, training_samples / 100)  # Score max √† 100 √©chantillons

                # Score final pond√©r√©
                final_score = (
                        mape_score * 0.4 +
                        stability_score * 0.2 +
                        complexity_score * 0.2 +
                        sample_score * 0.2
                )

                scores[model_name] = final_score

                logger.debug(f"{model_name}: MAPE={mape:.2f}%, Score={final_score:.3f}")

            # S√©lectionner le mod√®le avec le meilleur score
            if scores:
                best_model = max(scores.items(), key=lambda x: x[1])[0]
                logger.info(f"üèÜ Meilleur mod√®le s√©lectionn√©: {best_model} (score: {scores[best_model]:.3f})")
                return best_model
            else:
                return 'fallback'

        except Exception as e:
            logger.error(f"‚ùå Erreur s√©lection mod√®le: {e}")
            return list(model_results.keys())[0] if model_results else 'fallback'

    def _calculate_cache_duration(self, performance):
        """Calculer la dur√©e de cache bas√©e sur la performance"""
        try:
            mape = performance.get('mape', 50)
            if mape < 15:
                return 7200  # 2 heures pour excellente performance
            elif mape < 25:
                return 3600  # 1 heure pour bonne performance
            elif mape < 40:
                return 1800  # 30 minutes pour performance moyenne
            else:
                return 900  # 15 minutes pour performance faible
        except:
            return 1800

    def get_enhanced_contextual_data(self, blood_type):
        """
        üìà DONN√âES CONTEXTUELLES ENRICHIES
        """
        if not MODELS_AVAILABLE:
            return self._get_enhanced_default_contextual_data(blood_type)

        try:
            current_time = datetime.now()

            # Stock actuel d√©taill√©
            stock_data = BloodUnit.objects.filter(
                donor__blood_type=blood_type,
                status='Available'
            ).aggregate(
                total_units=Count('unit_id'),
                total_volume=Sum('volume_ml'),
                avg_expiry_days=Avg('days_until_expiry'),
                min_expiry_days=Count('unit_id', filter=Q(days_until_expiry__lte=7)),
                critical_expiry=Count('unit_id', filter=Q(days_until_expiry__lte=3))
            )

            # Demandes r√©centes avec d√©tails
            recent_requests = BloodRequest.objects.filter(
                blood_type=blood_type,
                request_date__gte=current_time - timedelta(days=14)
            ).aggregate(
                total_demand_2weeks=Sum('quantity'),
                total_demand_1week=Sum('quantity', filter=Q(request_date__gte=current_time - timedelta(days=7))),
                urgent_requests=Count('request_id', filter=Q(priority='Urgent')),
                emergency_requests=Count('request_id', filter=Q(priority='Emergency')),
                avg_daily_demand=Avg('quantity'),
                pending_requests=Count('request_id', filter=Q(status='Pending'))
            )

            # Consommation r√©cente
            try:
                recent_consumption = BloodConsumption.objects.filter(
                    unit__donor__blood_type=blood_type,
                    date__gte=current_time - timedelta(days=14)
                ).aggregate(
                    total_consumed_2weeks=Count('unit_id'),
                    total_consumed_1week=Count('unit_id', filter=Q(date__gte=current_time - timedelta(days=7))),
                    avg_daily_consumption=Avg('volume'),
                    peak_consumption_day=Count('unit_id')  # √Ä am√©liorer avec un group by
                )
            except:
                recent_consumption = {
                    'total_consumed_2weeks': 0,
                    'total_consumed_1week': 0,
                    'avg_daily_consumption': 0,
                    'peak_consumption_day': 0
                }

            # Tendances et patterns
            historical_trend = self._calculate_trend_indicators(blood_type, current_time)

            # Donn√©es sur les donneurs
            try:
                donor_data = Donor.objects.filter(blood_type=blood_type).aggregate(
                    active_donors=Count('donor_id', filter=Q(is_active=True)),
                    recent_donations=Count('donor_id',
                                           filter=Q(last_donation_date__gte=current_time - timedelta(days=30))),
                    avg_donation_frequency=Avg('donation_frequency')
                )
            except:
                donor_data = {'active_donors': 0, 'recent_donations': 0, 'avg_donation_frequency': 0}

            # Calculer des m√©triques d√©riv√©es
            current_stock = stock_data['total_units'] or 0
            weekly_demand = recent_requests['total_demand_1week'] or recent_consumption['total_consumed_1week'] or 0

            # Ratio d'urgence
            total_requests = (recent_requests['urgent_requests'] or 0) + (recent_requests['emergency_requests'] or 0)
            urgency_ratio = total_requests / max(1, recent_requests['total_demand_1week'] or 1)

            # Pression sur le stock
            stock_pressure = 1.0
            if current_stock > 0 and weekly_demand > 0:
                days_remaining = current_stock / max(1, weekly_demand / 7)
                stock_pressure = max(0, min(2, 1 + (7 - days_remaining) / 7))

            return {
                # Stock
                'current_stock': current_stock,
                'current_volume': stock_data['total_volume'] or 0,
                'avg_expiry_days': stock_data['avg_expiry_days'] or 30,
                'critical_expiry_units': stock_data['critical_expiry'] or 0,
                'near_expiry_units': stock_data['min_expiry_days'] or 0,

                # Demande
                'recent_weekly_demand': weekly_demand,
                'recent_2weeks_demand': recent_requests['total_demand_2weeks'] or 0,
                'daily_avg_demand': recent_requests['avg_daily_demand'] or 0,
                'pending_requests': recent_requests['pending_requests'] or 0,

                # Urgence
                'urgent_requests': recent_requests['urgent_requests'] or 0,
                'emergency_requests': recent_requests['emergency_requests'] or 0,
                'urgency_ratio': urgency_ratio,

                # Consommation
                'recent_weekly_consumption': recent_consumption['total_consumed_1week'] or 0,
                'recent_2weeks_consumption': recent_consumption['total_consumed_2weeks'] or 0,
                'avg_consumption_volume': recent_consumption['avg_daily_consumption'] or 0,

                # M√©triques d√©riv√©es
                'stock_pressure': stock_pressure,
                'days_stock_remaining': current_stock / max(1, weekly_demand / 7) if weekly_demand > 0 else 30,
                'demand_volatility': historical_trend.get('volatility', 0.2),
                'trend_direction': historical_trend.get('trend', 0),

                # Donneurs
                'active_donors': donor_data['active_donors'] or 0,
                'recent_donations': donor_data['recent_donations'] or 0,
                'donation_capacity': donor_data['avg_donation_frequency'] or 1,

                # M√©tadonn√©es
                'data_freshness': 'real_time',
                'last_updated': current_time.isoformat(),
                'completeness_score': self._calculate_data_completeness(stock_data, recent_requests, recent_consumption)
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur donn√©es contextuelles enrichies: {e}")
            return self._get_enhanced_default_contextual_data(blood_type)

    def _calculate_trend_indicators(self, blood_type, current_time):
        """Calculer les indicateurs de tendance"""
        try:
            # R√©cup√©rer les donn√©es des 30 derniers jours
            if MODELS_AVAILABLE:
                daily_demands = BloodRequest.objects.filter(
                    blood_type=blood_type,
                    request_date__gte=current_time - timedelta(days=30),
                    status__in=['Fulfilled', 'Approved']
                ).extra(
                    select={'day': 'DATE(request_date)'}
                ).values('day').annotate(
                    daily_total=Sum('quantity')
                ).order_by('day')

                if daily_demands.exists():
                    demands = [d['daily_total'] for d in daily_demands]

                    # Calcul de la tendance (r√©gression lin√©aire simple)
                    if len(demands) >= 7:
                        x = np.arange(len(demands))
                        trend_slope = np.polyfit(x, demands, 1)[0]
                        volatility = np.std(demands) / max(1, np.mean(demands))
                    else:
                        trend_slope = 0
                        volatility = 0.2

                    return {
                        'trend': trend_slope,
                        'volatility': volatility,
                        'mean_demand': np.mean(demands),
                        'recent_peak': max(demands) if demands else 0
                    }

            # Fallback
            config = self.blood_type_config.get(blood_type, {})
            return {
                'trend': 0,
                'volatility': config.get('volatility', 0.2),
                'mean_demand': config.get('base_demand', 5),
                'recent_peak': config.get('base_demand', 5) * 1.5
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur calcul tendances: {e}")
            return {'trend': 0, 'volatility': 0.2, 'mean_demand': 5, 'recent_peak': 7}

    def _calculate_data_completeness(self, stock_data, request_data, consumption_data):
        """Calculer un score de compl√©tude des donn√©es"""
        try:
            score = 0
            total_checks = 10

            # V√©rifications de compl√©tude
            if stock_data.get('total_units', 0) > 0: score += 1
            if stock_data.get('avg_expiry_days') is not None: score += 1
            if request_data.get('total_demand_1week', 0) > 0: score += 1
            if request_data.get('avg_daily_demand') is not None: score += 1
            if consumption_data.get('total_consumed_1week', 0) > 0: score += 1
            if consumption_data.get('avg_daily_consumption') is not None: score += 1

            # Bonus pour donn√©es r√©centes
            if request_data.get('urgent_requests', 0) >= 0: score += 1
            if request_data.get('pending_requests', 0) >= 0: score += 1
            if stock_data.get('critical_expiry', 0) >= 0: score += 1
            if consumption_data.get('total_consumed_2weeks', 0) >= 0: score += 1

            return min(1.0, score / total_checks)

        except:
            return 0.5

    def _get_enhanced_default_contextual_data(self, blood_type):
        """Donn√©es contextuelles par d√©faut am√©lior√©es"""
        config = self.blood_type_config.get(blood_type, {})
        base_demand = config.get('base_demand', 5)

        return {
            'current_stock': base_demand * 4,
            'current_volume': base_demand * 4 * 450,
            'avg_expiry_days': 25,
            'critical_expiry_units': 1,
            'near_expiry_units': 2,
            'recent_weekly_demand': base_demand * 7,
            'daily_avg_demand': base_demand,
            'urgent_requests': 1 if config.get('priority') == 'critical' else 0,
            'emergency_requests': 0,
            'urgency_ratio': 0.1,
            'stock_pressure': 1.0,
            'days_stock_remaining': 4,
            'demand_volatility': config.get('volatility', 0.2),
            'trend_direction': 0,
            'data_freshness': 'default_config',
            'completeness_score': 0.3
        }

    def _create_enhanced_fallback_model(self, blood_type):
        """Mod√®le de secours am√©lior√©"""
        try:
            config = self.blood_type_config.get(blood_type, {})
            base_demand = config.get('base_demand', 5)
            volatility = config.get('volatility', 0.2)

            # Fallback plus intelligent bas√© sur des patterns r√©alistes
            fallback_performance = {
                'enhanced_fallback': {
                    'mae': base_demand * 0.2,
                    'rmse': base_demand * 0.3,
                    'mape': min(35.0, 20 + volatility * 50),  # MAPE adaptatif
                    'training_samples': 90,
                    'model_type': 'rule_based_enhanced',
                    'confidence_level': 0.6,
                    'is_fallback': True
                }
            }

            self.model_performance[blood_type] = fallback_performance
            logger.info(f"‚úÖ Mod√®le fallback am√©lior√© cr√©√© pour {blood_type}")
            return fallback_performance, 'enhanced_fallback'

        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation fallback am√©lior√©: {e}")
            return {}, 'error'

    def predict_with_enhanced_models(self, blood_type, days_ahead=7, method='auto'):
        """
        üîÆ PR√âDICTION AVEC MOD√àLES AM√âLIOR√âS
        """
        cache_key = f'enhanced_prediction_{blood_type}_{days_ahead}_{method}'
        cached = cache.get(cache_key)
        if cached:
            logger.info(f"‚úÖ Pr√©diction am√©lior√©e en cache pour {blood_type}")
            return cached

        self.start_time = time.time()

        try:
            # Entra√Æner tous les mod√®les
            performance, best_method = self.train_all_models_improved(blood_type, method)

            if not performance:
                return self.enhanced_emergency_fallback_prediction(blood_type, days_ahead)

            # Utiliser la meilleure m√©thode ou celle sp√©cifi√©e
            final_method = best_method if method == 'auto' else method
            if final_method not in performance:
                final_method = best_method

            # G√©n√©rer les pr√©dictions selon la m√©thode
            predictions = self.generate_enhanced_predictions(blood_type, days_ahead, final_method)

            if not predictions:
                return self.enhanced_emergency_fallback_prediction(blood_type, days_ahead)

            # Donn√©es contextuelles enrichies
            contextual_data = self.get_enhanced_contextual_data(blood_type)

            # Calcul des intervalles de confiance am√©lior√©s
            confidence_intervals = self.calculate_enhanced_confidence_intervals(
                predictions, performance.get(final_method, {})
            )

            # M√©triques de qualit√© avanc√©es
            quality_metrics = self.calculate_enhanced_quality_metrics(
                predictions, performance.get(final_method, {}), contextual_data
            )

            # Insights contextuels intelligents
            contextual_insights = self.generate_contextual_insights(
                contextual_data, predictions, blood_type
            )

            result = {
                'blood_type': blood_type,
                'predictions': predictions,
                'method_used': final_method,
                'model_performance': performance.get(final_method, {}),
                'confidence_intervals': confidence_intervals,
                'generated_at': datetime.now().isoformat(),
                'data_source': 'enhanced_real_database',
                'contextual_insights': contextual_insights,
                'quality_metrics': quality_metrics,
                'feature_importance': self.feature_importance.get(blood_type, {}).get(final_method, {}),
                'alternative_methods': {
                    method: perf.get('mape', 100)
                    for method, perf in performance.items()
                    if method != final_method
                },
                'system_info': {
                    'models_available': MODELS_AVAILABLE,
                    'xgboost_available': XGBOOST_AVAILABLE,
                    'statsmodels_available': STATSMODELS_AVAILABLE,
                    'lightgbm_available': LIGHTGBM_AVAILABLE,
                    'data_points_used': performance.get(final_method, {}).get('training_samples', 0)
                }
            }

            # Cache adaptatif selon performance et stabilit√©
            cache_duration = self._calculate_enhanced_cache_duration(
                performance.get(final_method, {}), quality_metrics
            )
            cache.set(cache_key, result, cache_duration)

            # Log d√©taill√© du succ√®s
            mape = performance.get(final_method, {}).get('mape', 0)
            confidence = quality_metrics.get('prediction_confidence', 0)
            data_quality = quality_metrics.get('data_quality_score', 0)

            logger.info(f"‚úÖ Pr√©diction am√©lior√©e g√©n√©r√©e pour {blood_type}: "
                        f"M√©thode={final_method}, MAPE={mape:.1f}%, "
                        f"Confiance={confidence:.1f}, Qualit√©={data_quality:.1f}")

            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction am√©lior√©e: {e}")
            return self.enhanced_emergency_fallback_prediction(blood_type, days_ahead)

    def generate_enhanced_predictions(self, blood_type, days_ahead, method):
        """G√©n√©ration de pr√©dictions am√©lior√©es"""
        try:
            if method in ['random_forest', 'gradient_boosting', 'linear_regression', 'xgboost', 'lightgbm', 'ensemble']:
                return self.predict_ml_enhanced(blood_type, days_ahead, method)
            elif method in ['arima', 'stl_arima', 'exponential_smoothing']:
                return self.predict_time_series_enhanced(blood_type, days_ahead, method)
            elif method in ['enhanced_fallback', 'fallback']:
                return self.predict_enhanced_fallback(blood_type, days_ahead)
            else:
                logger.warning(f"‚ö†Ô∏è M√©thode inconnue: {method}")
                return self.predict_enhanced_fallback(blood_type, days_ahead)

        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration pr√©dictions am√©lior√©es: {e}")
            return self.predict_enhanced_fallback(blood_type, days_ahead)

    def predict_ml_enhanced(self, blood_type, days_ahead, method):
        """Pr√©diction ML am√©lior√©e"""
        model_key = f"{method}_{blood_type}"

        if model_key not in self.trained_models:
            logger.error(f"‚ùå Mod√®le {model_key} non trouv√©")
            return self.predict_enhanced_fallback(blood_type, days_ahead)

        try:
            model_data = self.trained_models[model_key]

            if method == 'ensemble':
                return self._predict_ensemble_enhanced(blood_type, days_ahead, model_data)
            else:
                return self._predict_single_ml_enhanced(blood_type, days_ahead, model_data)

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction ML: {e}")
            return self.predict_enhanced_fallback(blood_type, days_ahead)

    def _predict_single_ml_enhanced(self, blood_type, days_ahead, model_data):
        """Pr√©diction ML simple am√©lior√©e"""
        try:
            model = model_data['model']
            feature_cols = model_data['features']
            scaler = model_data.get('scaler')

            # R√©cup√©rer les donn√©es r√©centes pour construire les features
            recent_data = self.get_enhanced_historical_data(blood_type, days_back=60)
            if recent_data is None:
                return self.predict_enhanced_fallback(blood_type, days_ahead)

            contextual_data = self.get_enhanced_contextual_data(blood_type)
            df_with_features = self.prepare_advanced_features(recent_data, contextual_data)

            if df_with_features is None:
                return self.predict_enhanced_fallback(blood_type, days_ahead)

            predictions = []
            last_known_values = df_with_features['demand'].tail(21).values  # 3 semaines

            # Variables pour tracking de la qualit√© des pr√©dictions
            prediction_confidence = 0.8
            uncertainty_accumulation = 0.02

            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)

                # Construction des features futures am√©lior√©es
                future_features = self.build_enhanced_future_features(
                    future_date, df_with_features, last_known_values, i, contextual_data
                )

                if len(future_features) != len(feature_cols):
                    # Fallback pour cette pr√©diction
                    fallback_pred = self.predict_enhanced_fallback(blood_type, 1)
                    if fallback_pred:
                        pred = fallback_pred[0]['predicted_demand']
                        confidence = fallback_pred[0]['confidence'] * 0.8
                    else:
                        config = self.blood_type_config.get(blood_type, {})
                        pred = config.get('base_demand', 5)
                        confidence = 0.4
                else:
                    # Pr√©diction ML
                    if scaler:
                        features_scaled = scaler.transform([future_features])
                        pred = model.predict(features_scaled)[0]
                    else:
                        pred = model.predict([future_features])[0]

                    pred = max(0, int(round(pred)))

                    # Calcul de confiance am√©lior√©
                    confidence = self._calculate_prediction_confidence(
                        last_known_values, pred, i, prediction_confidence, uncertainty_accumulation
                    )

                # Ajustements contextuels
                pred, confidence = self._apply_contextual_adjustments(
                    pred, confidence, future_date, contextual_data, blood_type
                )

                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': pred,
                    'confidence': round(confidence, 3),
                    'prediction_method': 'ml_enhanced',
                    'contextual_factors': self._get_prediction_factors(future_date, contextual_data),
                    'uncertainty_level': min(0.5, i * uncertainty_accumulation)
                })

                # Mise √† jour des valeurs connues
                if len(last_known_values) >= 21:
                    last_known_values = np.append(last_known_values[1:], pred)
                else:
                    last_known_values = np.append(last_known_values, pred)

                # D√©cr√©menter la confiance avec le temps
                prediction_confidence *= 0.98

            return predictions

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction ML simple: {e}")
            return self.predict_enhanced_fallback(blood_type, days_ahead)

    def _predict_ensemble_enhanced(self, blood_type, days_ahead, ensemble_data):
        """Pr√©diction d'ensemble am√©lior√©e"""
        try:
            models_list = ensemble_data['models']
            weights = ensemble_data['weights']
            feature_cols = ensemble_data['features']

            # R√©cup√©rer les donn√©es pour les features
            recent_data = self.get_enhanced_historical_data(blood_type, days_back=60)
            contextual_data = self.get_enhanced_contextual_data(blood_type)
            df_with_features = self.prepare_advanced_features(recent_data, contextual_data)

            if df_with_features is None:
                return self.predict_enhanced_fallback(blood_type, days_ahead)

            predictions = []
            last_known_values = df_with_features['demand'].tail(21).values

            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)

                future_features = self.build_enhanced_future_features(
                    future_date, df_with_features, last_known_values, i, contextual_data
                )

                ensemble_pred = 0
                ensemble_confidence = 0
                valid_predictions = 0

                # Pr√©diction de chaque mod√®le de l'ensemble
                for j, model_name in enumerate(models_list):
                    model_key = f"{model_name}_{blood_type}"
                    if model_key in self.trained_models:
                        try:
                            model_data = self.trained_models[model_key]
                            model = model_data['model']
                            scaler = model_data.get('scaler')

                            if scaler:
                                features_scaled = scaler.transform([future_features])
                                pred = model.predict(features_scaled)[0]
                            else:
                                pred = model.predict([future_features])[0]

                            pred = max(0, pred)
                            weight = weights[j]

                            ensemble_pred += weight * pred
                            ensemble_confidence += weight * 0.8  # Confiance de base pour chaque mod√®le
                            valid_predictions += 1

                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Erreur mod√®le {model_name} dans ensemble: {e}")
                            continue

                if valid_predictions == 0:
                    # Fallback si aucun mod√®le n'a fonctionn√©
                    fallback_pred = self.predict_enhanced_fallback(blood_type, 1)
                    if fallback_pred:
                        ensemble_pred = fallback_pred[0]['predicted_demand']
                        ensemble_confidence = fallback_pred[0]['confidence']
                    else:
                        config = self.blood_type_config.get(blood_type, {})
                        ensemble_pred = config.get('base_demand', 5)
                        ensemble_confidence = 0.5
                else:
                    # Normaliser la confiance par le nombre de mod√®les valides
                    ensemble_confidence = ensemble_confidence * (valid_predictions / len(models_list))

                ensemble_pred = max(0, int(round(ensemble_pred)))

                # Ajustements contextuels
                ensemble_pred, ensemble_confidence = self._apply_contextual_adjustments(
                    ensemble_pred, ensemble_confidence, future_date, contextual_data, blood_type
                )

                # Bonus de confiance pour l'ensemble
                ensemble_confidence = min(0.95, ensemble_confidence * 1.1)

                # D√©cr√©menter la confiance avec la distance temporelle
                time_decay = 0.98 ** i
                final_confidence = ensemble_confidence * time_decay

                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': ensemble_pred,
                    'confidence': round(final_confidence, 3),
                    'prediction_method': 'ensemble_enhanced',
                    'models_used': len(models_list),
                    'valid_models': valid_predictions,
                    'ensemble_weights': weights,
                    'contextual_factors': self._get_prediction_factors(future_date, contextual_data)
                })

                # Mise √† jour des valeurs connues
                if len(last_known_values) >= 21:
                    last_known_values = np.append(last_known_values[1:], ensemble_pred)
                else:
                    last_known_values = np.append(last_known_values, ensemble_pred)

            return predictions

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction ensemble: {e}")
            return self.predict_enhanced_fallback(blood_type, days_ahead)

    def predict_time_series_enhanced(self, blood_type, days_ahead, method):
        """Pr√©diction time series am√©lior√©e"""
        model_key = f"{method}_{blood_type}"

        if model_key not in self.arima_models:
            logger.error(f"‚ùå Mod√®le time series {model_key} non trouv√©")
            return self.predict_enhanced_fallback(blood_type, days_ahead)

        try:
            model_data = self.arima_models[model_key]
            contextual_data = self.get_enhanced_contextual_data(blood_type)

            if method == 'arima':
                return self._predict_arima_enhanced(blood_type, days_ahead, model_data, contextual_data)
            elif method == 'stl_arima':
                return self._predict_stl_arima_enhanced(blood_type, days_ahead, model_data, contextual_data)
            elif method == 'exponential_smoothing':
                return self._predict_exp_smooth_enhanced(blood_type, days_ahead, model_data, contextual_data)
            else:
                return self.predict_enhanced_fallback(blood_type, days_ahead)

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction time series: {e}")
            return self.predict_enhanced_fallback(blood_type, days_ahead)

    def _predict_arima_enhanced(self, blood_type, days_ahead, model_data, contextual_data):
        """Pr√©diction ARIMA am√©lior√©e"""
        try:
            arima_model = model_data['model']
            order = model_data['order']

            # Pr√©diction ARIMA de base
            forecast = arima_model.forecast(steps=days_ahead)
            forecast_ci = arima_model.get_forecast(steps=days_ahead).conf_int()

            predictions = []
            base_confidence = 0.75

            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)

                pred = max(0, int(round(forecast.iloc[i])))

                # Calcul de confiance bas√© sur l'intervalle de confiance
                ci_lower = forecast_ci.iloc[i, 0]
                ci_upper = forecast_ci.iloc[i, 1]
                ci_width = ci_upper - ci_lower

                # Confiance inversement proportionnelle √† la largeur de l'intervalle
                confidence = base_confidence * (1 / (1 + ci_width / max(1, pred)))
                confidence = min(0.9, confidence * (0.98 ** i))  # D√©croissance temporelle

                # Ajustements contextuels
                pred, confidence = self._apply_contextual_adjustments(
                    pred, confidence, future_date, contextual_data, blood_type
                )

                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': pred,
                    'confidence': round(confidence, 3),
                    'prediction_method': 'arima_enhanced',
                    'arima_order': order,
                    'confidence_interval': {
                        'lower': max(0, int(ci_lower)),
                        'upper': int(ci_upper)
                    },
                    'contextual_factors': self._get_prediction_factors(future_date, contextual_data)
                })

            return predictions

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction ARIMA: {e}")
            return self.predict_enhanced_fallback(blood_type, days_ahead)

    def _predict_stl_arima_enhanced(self, blood_type, days_ahead, model_data, contextual_data):
        """Pr√©diction STL+ARIMA am√©lior√©e"""
        try:
            stl_result = model_data['stl_model']
            arima_model = model_data['arima_model']

            # Extraire les composantes
            trend = stl_result.trend
            seasonal = stl_result.seasonal

            # Pr√©diction des r√©sidus avec ARIMA
            residual_forecast = arima_model.forecast(steps=days_ahead)

            predictions = []
            base_confidence = 0.8

            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)

                # Extrapoler la tendance
                if len(trend) >= 7:
                    trend_pred = trend.iloc[-1] + (trend.iloc[-1] - trend.iloc[-7]) * (i + 1) / 7
                else:
                    trend_pred = trend.iloc[-1]

                # Pr√©diction saisonni√®re cyclique
                seasonal_period = 7  # Hebdomadaire
                seasonal_idx = (-1 - i) % seasonal_period
                seasonal_pred = seasonal.iloc[seasonal_idx] if len(seasonal) > seasonal_period else 0

                # Combinaison finale
                pred = trend_pred + seasonal_pred + residual_forecast.iloc[i]
                pred = max(0, int(round(pred)))

                # Confiance d√©croissante avec le temps
                confidence = base_confidence * (0.97 ** i)

                # Ajustements contextuels
                pred, confidence = self._apply_contextual_adjustments(
                    pred, confidence, future_date, contextual_data, blood_type
                )

                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': pred,
                    'confidence': round(confidence, 3),
                    'prediction_method': 'stl_arima_enhanced',
                    'decomposition': {
                        'trend': round(trend_pred, 2),
                        'seasonal': round(seasonal_pred, 2),
                        'residual': round(residual_forecast.iloc[i], 2)
                    },
                    'contextual_factors': self._get_prediction_factors(future_date, contextual_data)
                })

            return predictions

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction STL-ARIMA: {e}")
            return self.predict_enhanced_fallback(blood_type, days_ahead)

    def _predict_exp_smooth_enhanced(self, blood_type, days_ahead, model_data, contextual_data):
        """Pr√©diction lissage exponentiel am√©lior√©e"""
        try:
            exp_model = model_data['model']
            config = model_data['config']

            # Pr√©diction avec lissage exponentiel
            forecast = exp_model.forecast(steps=days_ahead)

            predictions = []
            base_confidence = 0.75

            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)

                pred = max(0, int(round(forecast.iloc[i])))

                # Confiance bas√©e sur la m√©thode de lissage
                if config.get('seasonal') == 'add':
                    confidence = base_confidence * 1.1  # Bonus pour saisonnalit√© additive
                elif config.get('seasonal') == 'mul':
                    confidence = base_confidence * 1.05  # L√©ger bonus pour saisonnalit√© multiplicative
                else:
                    confidence = base_confidence

                confidence *= (0.98 ** i)  # D√©croissance temporelle

                # Ajustements contextuels
                pred, confidence = self._apply_contextual_adjustments(
                    pred, confidence, future_date, contextual_data, blood_type
                )

                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': pred,
                    'confidence': round(confidence, 3),
                    'prediction_method': 'exponential_smoothing_enhanced',
                    'smoothing_config': config,
                    'contextual_factors': self._get_prediction_factors(future_date, contextual_data)
                })

            return predictions

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction lissage exponentiel: {e}")
            return self.predict_enhanced_fallback(blood_type, days_ahead)

    def predict_enhanced_fallback(self, blood_type, days_ahead):
        """Pr√©diction fallback am√©lior√©e"""
        try:
            logger.info(f"üö® Utilisation fallback am√©lior√© pour {blood_type}")

            config = self.blood_type_config.get(blood_type, {})
            base_demand = config.get('base_demand', 5)
            weekend_factor = config.get('typical_weekend_factor', 0.8)
            seasonality_strength = config.get('seasonality_strength', 0.2)
            volatility = config.get('volatility', 0.2)

            # Essayer de r√©cup√©rer des donn√©es r√©centes pour calibration
            contextual_data = self.get_enhanced_contextual_data(blood_type)
            recent_avg = contextual_data.get('daily_avg_demand', 0)

            if recent_avg > 0:
                # Ajuster base_demand avec les donn√©es r√©centes
                adjustment_factor = recent_avg / base_demand
                adjustment_factor = max(0.5, min(2.0, adjustment_factor))
                base_demand = int(base_demand * adjustment_factor)

            # R√©cup√©rer un pattern hebdomadaire plus r√©aliste
            weekly_pattern = self._generate_weekly_pattern(blood_type, base_demand, contextual_data)

            predictions = []

            for i in range(days_ahead):
                future_date = datetime.now() + timedelta(days=i + 1)
                day_of_week = future_date.weekday()

                # Base avec pattern hebdomadaire
                if len(weekly_pattern) > day_of_week:
                    daily_base = weekly_pattern[day_of_week]
                else:
                    daily_base = base_demand * (weekend_factor if day_of_week in [5, 6] else 1.0)

                # Ajustements saisonniers
                seasonal_adj = self._calculate_seasonal_adjustment(future_date, seasonality_strength)
                daily_base *= seasonal_adj

                # Ajustements contextuels bas√©s sur l'√©tat du stock
                context_adj = self._calculate_context_adjustment(contextual_data, i)
                daily_base *= context_adj

                # Variation r√©aliste
                noise = np.random.normal(0, volatility * 0.5)  # Moins de bruit pour plus de stabilit√©
                final_demand = max(1, int(daily_base * (1 + noise)))

                # Confiance am√©lior√©e
                base_confidence = 0.65 if contextual_data.get('completeness_score', 0) > 0.5 else 0.5
                confidence = base_confidence * (0.98 ** i)

                # Bonus de confiance si on a des donn√©es r√©centes
                if recent_avg > 0:
                    confidence *= 1.1

                predictions.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_demand': final_demand,
                    'confidence': round(min(0.8, confidence), 3),
                    'prediction_method': 'enhanced_fallback',
                    'adjustments': {
                        'weekly_pattern': round(daily_base / base_demand, 2),
                        'seasonal': round(seasonal_adj, 2),
                        'contextual': round(context_adj, 2)
                    },
                    'contextual_factors': self._get_prediction_factors(future_date, contextual_data)
                })

            return predictions

        except Exception as e:
            logger.error(f"‚ùå Erreur fallback am√©lior√©: {e}")
            # Fallback du fallback
            return self._generate_ultra_simple_predictions(blood_type, days_ahead)

    def _generate_weekly_pattern(self, blood_type, base_demand, contextual_data):
        """G√©n√©rer un pattern hebdomadaire intelligent"""
        try:
            # Pattern par d√©faut bas√© sur la logique m√©dicale
            default_pattern = [
                1.2,  # Lundi - plus de demande apr√®s le weekend
                1.0,  # Mardi - normal
                1.0,  # Mercredi - normal
                1.1,  # Jeudi - l√©g√®rement plus
                1.15,  # Vendredi - plus avant le weekend
                0.7,  # Samedi - moins
                0.6  # Dimanche - le moins
            ]

            config = self.blood_type_config.get(blood_type, {})
            weekend_factor = config.get('typical_weekend_factor', 0.8)

            # Ajuster le pattern avec le weekend_factor
            pattern = []
            for i, factor in enumerate(default_pattern):
                if i in [5, 6]:  # Weekend
                    adjusted_factor = factor * (weekend_factor / 0.65)  # Normaliser
                else:
                    adjusted_factor = factor
                pattern.append(base_demand * adjusted_factor)

            return pattern

        except:
            # Pattern ultra-simple en cas d'erreur
            weekend_factor = 0.8
            return [base_demand * (weekend_factor if i in [5, 6] else 1.0) for i in range(7)]

    def _calculate_seasonal_adjustment(self, date, seasonality_strength):
        """Calculer l'ajustement saisonnier"""
        try:
            day_of_year = date.timetuple().tm_yday

            # Composante annuelle (plus de demande en hiver)
            annual_cycle = 1 + seasonality_strength * np.sin(2 * np.pi * (day_of_year - 80) / 365)

            # Composante mensuelle (d√©but/fin de mois)
            monthly_cycle = 1 + (seasonality_strength * 0.1) * np.sin(2 * np.pi * date.day / 30)

            return annual_cycle * monthly_cycle

        except:
            return 1.0

    def _calculate_context_adjustment(self, contextual_data, day_offset):
        """Calculer l'ajustement contextuel"""
        try:
            adjustment = 1.0

            # Ajustement bas√© sur la pression du stock
            stock_pressure = contextual_data.get('stock_pressure', 1.0)
            if stock_pressure > 1.2:
                adjustment *= 1.1  # Plus de demande si stock sous pression
            elif stock_pressure < 0.8:
                adjustment *= 0.95  # Moins de demande si stock abondant

            # Ajustement bas√© sur les demandes urgentes
            urgency_ratio = contextual_data.get('urgency_ratio', 0)
            if urgency_ratio > 0.2:
                adjustment *= 1.05  # L√©g√®re augmentation si beaucoup d'urgences

            # Ajustement temporel (effet diminuant dans le temps)
            time_decay = 0.99 ** day_offset
            adjustment = 1.0 + (adjustment - 1.0) * time_decay

            return max(0.8, min(1.3, adjustment))

        except:
            return 1.0

    def _get_prediction_factors(self, future_date, contextual_data):
        """Obtenir les facteurs influen√ßant la pr√©diction"""
        try:
            factors = {
                'day_of_week': future_date.strftime('%A'),
                'is_weekend': future_date.weekday() in [5, 6],
                'month': future_date.strftime('%B'),
                'stock_level': 'low' if contextual_data.get('stock_pressure', 1) > 1.2 else
                'high' if contextual_data.get('stock_pressure', 1) < 0.8 else 'normal',
                'urgent_requests': contextual_data.get('urgent_requests', 0),
                'trend': 'increasing' if contextual_data.get('trend_direction', 0) > 0.1 else
                'decreasing' if contextual_data.get('trend_direction', 0) < -0.1 else 'stable'
            }

            return factors

        except:
            return {'day_of_week': future_date.strftime('%A'), 'is_weekend': future_date.weekday() in [5, 6]}

    def _generate_ultra_simple_predictions(self, blood_type, days_ahead):
        """G√©n√©rateur de pr√©dictions ultra-simple en dernier recours"""
        try:
            config = self.blood_type_config.get(blood_type, {})
            base = max(1, config.get('base_demand', 3))

            predictions = []
            for i in range(days_ahead):
                date = datetime.now() + timedelta(days=i + 1)
                weekend_factor = 0.8 if date.weekday() in [5, 6] else 1.0
                demand = max(1, int(base * weekend_factor))

                predictions.append({
                    'date': date.strftime('%Y-%m-%d'),
                    'predicted_demand': demand,
                    'confidence': 0.4,
                    'prediction_method': 'ultra_simple_fallback'
                })

            return predictions

        except:
            # Fallback absolu
            return [{
                'date': (datetime.now() + timedelta(days=i + 1)).strftime('%Y-%m-%d'),
                'predicted_demand': 3,
                'confidence': 0.3,
                'prediction_method': 'absolute_fallback'
            } for i in range(days_ahead)]

    def build_enhanced_future_features(self, future_date, historical_df, last_values, day_offset, contextual_data):
        """Construction de features futures am√©lior√©es"""
        try:
            features = []

            # Features temporelles de base
            features.extend([
                future_date.weekday(),
                future_date.month,
                future_date.day,
                future_date.quarter,
                future_date.isocalendar().week,
                1 if future_date.weekday() in [5, 6] else 0,
                1 if future_date.weekday() == 0 else 0,
                1 if future_date.weekday() == 4 else 0,
                1 if future_date.day <= 5 else 0,
                1 if future_date.day >= 25 else 0,
            ])

            # Moyennes mobiles sur les derni√®res valeurs
            mean_demand = np.mean(last_values) if len(last_values) > 0 else 5

            for window in [3, 7, 14, 30]:
                if len(last_values) >= window:
                    ma = np.mean(last_values[-window:])
                    std = np.std(last_values[-window:])
                else:
                    ma = mean_demand
                    std = mean_demand * 0.2
                features.extend([ma, std])

            # Moyennes mobiles exponentielles
            for alpha in [0.1, 0.3, 0.5]:
                if len(last_values) > 0:
                    ema = last_values[-1]  # Simplification
                    for val in last_values[-10:]:
                        ema = alpha * val + (1 - alpha) * ema
                    features.append(ema)
                else:
                    features.append(mean_demand)

            # Lags
            for lag in [1, 2, 3, 7, 14]:
                if len(last_values) >= lag:
                    features.append(last_values[-lag])
                else:
                    features.append(mean_demand)

            # Tendances
            for window in [7, 14, 30]:
                if len(last_values) >= window:
                    x = np.arange(window)
                    y = last_values[-window:]
                    if len(y) >= 3:
                        trend = np.polyfit(x, y, 1)[0]
                    else:
                        trend = 0
                else:
                    trend = 0
                features.append(trend)

            # Volatilit√©
            for window in [7, 14]:
                if len(last_values) >= window:
                    volatility = np.std(last_values[-window:])
                else:
                    volatility = mean_demand * 0.2
                features.append(volatility)

            # Features cycliques
            features.extend([
                np.sin(2 * np.pi * future_date.weekday() / 7),
                np.cos(2 * np.pi * future_date.weekday() / 7),
                np.sin(2 * np.pi * future_date.month / 12),
                np.cos(2 * np.pi * future_date.month / 12),
                np.sin(2 * np.pi * future_date.day / 31),
                np.cos(2 * np.pi * future_date.day / 31),
            ])

            # Features d'interaction
            features.extend([
                (1 if future_date.weekday() in [5, 6] else 0) * future_date.month,
                (1 if future_date.weekday() == 0 else 0) * future_date.quarter,
            ])

            # Features de diff√©rences et variations
            if len(last_values) >= 2:
                features.extend([
                    last_values[-1] - last_values[-2],  # Diff√©rence
                    (last_values[-1] - last_values[-2]) / max(1, last_values[-2])  # Variation relative
                ])
            else:
                features.extend([0, 0])

            # Features quantiles
            if len(last_values) >= 10:
                q25, q75 = np.percentile(last_values[-30:] if len(last_values) >= 30 else last_values, [25, 75])
                features.extend([
                    q25, q75,
                    1 if len(last_values) > 0 and last_values[-1] > q75 else 0,
                    1 if len(last_values) > 0 and last_values[-1] < q25 else 0
                ])
            else:
                q25, q75 = mean_demand * 0.8, mean_demand * 1.2
                features.extend([q25, q75, 0, 0])

            # Features contextuelles enrichies
            if contextual_data:
                features.extend([
                    contextual_data.get('stock_ratio', 1.0),
                    contextual_data.get('recent_trend_factor', 1.0),
                    contextual_data.get('urgent_factor', 0.0),
                    contextual_data.get('expiry_pressure', 0.0),
                    contextual_data.get('stock_pressure', 1.0),
                    contextual_data.get('urgency_ratio', 0.0),
                    contextual_data.get('demand_volatility', 0.2)
                ])
            else:
                features.extend([1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2])

            # Features saisonni√®res avanc√©es
            season = future_date.month % 12 // 3
            features.extend([
                season,
                1 if season == 2 else 0,  # √©t√©
                1 if season == 0 else 0,  # hiver
            ])

            return features

        except Exception as e:
            logger.error(f"‚ùå Erreur construction features am√©lior√©es: {e}")
            # Retourner des features minimales en cas d'erreur
            return [future_date.weekday(), future_date.month, 1 if future_date.weekday() in [5, 6] else 0] + [5.0] * 20

    def _calculate_prediction_confidence(self, last_values, prediction, day_offset, base_confidence,
                                         uncertainty_accumulation):
        """Calculer la confiance d'une pr√©diction"""
        try:
            confidence = base_confidence

            # Ajustement bas√© sur la stabilit√© r√©cente
            if len(last_values) >= 7:
                recent_std = np.std(last_values[-7:])
                recent_mean = np.mean(last_values[-7:])
                stability_factor = max(0.5, 1 - (recent_std / max(1, recent_mean)))
                confidence *= stability_factor

            # Ajustement bas√© sur la coh√©rence de la pr√©diction
            if len(last_values) > 0:
                recent_avg = np.mean(last_values[-3:]) if len(last_values) >= 3 else last_values[-1]
                prediction_deviation = abs(prediction - recent_avg) / max(1, recent_avg)
                if prediction_deviation > 0.5:  # Pr√©diction tr√®s diff√©rente
                    confidence *= 0.8
                elif prediction_deviation < 0.2:  # Pr√©diction coh√©rente
                    confidence *= 1.1

            # D√©croissance temporelle
            confidence *= (0.98 ** day_offset)

            return max(0.1, min(0.95, confidence))

        except:
            return max(0.3, base_confidence * (0.98 ** day_offset))

    def _apply_contextual_adjustments(self, prediction, confidence, future_date, contextual_data, blood_type):
        """Appliquer des ajustements contextuels √† la pr√©diction"""
        try:
            adjusted_pred = prediction
            adjusted_conf = confidence

            # Ajustement bas√© sur le stock
            stock_pressure = contextual_data.get('stock_pressure', 1.0)
            if stock_pressure > 1.3:
                # Stock tr√®s bas, augmenter la pr√©diction
                adjusted_pred = int(adjusted_pred * 1.1)
                adjusted_conf *= 0.9  # Moins de confiance car situation anormale
            elif stock_pressure < 0.7:
                # Stock tr√®s haut, diminuer l√©g√®rement
                adjusted_pred = int(adjusted_pred * 0.95)

            # Ajustement pour les demandes urgentes
            urgency_ratio = contextual_data.get('urgency_ratio', 0)
            if urgency_ratio > 0.3:
                adjusted_pred = int(adjusted_pred * 1.05)

            # Ajustement pour les jours critiques (lundi, vendredi)
            if future_date.weekday() == 0:  # Lundi
                adjusted_pred = int(adjusted_pred * 1.05)
            elif future_date.weekday() == 4:  # Vendredi
                adjusted_pred = int(adjusted_pred * 1.03)

            # Ajustement pour les types sanguins critiques
            config = self.blood_type_config.get(blood_type, {})
            if config.get('priority') == 'critical':
                adjusted_pred = max(adjusted_pred, 2)  # Minimum de s√©curit√©
                adjusted_conf *= 1.05  # L√©g√®rement plus de confiance

            # S'assurer que la pr√©diction reste positive
            adjusted_pred = max(1, adjusted_pred)
            adjusted_conf = max(0.1, min(0.95, adjusted_conf))

            return adjusted_pred, adjusted_conf

        except Exception as e:
            logger.error(f"‚ùå Erreur ajustements contextuels: {e}")
            return max(1, prediction), max(0.1, confidence)

    def calculate_enhanced_confidence_intervals(self, predictions, performance):
        """Calculer des intervalles de confiance am√©lior√©s"""
        try:
            if not predictions:
                return {'lower': [], 'upper': [], 'margin': 0, 'method': 'none'}

            demands = [p['predicted_demand'] for p in predictions]
            confidences = [p['confidence'] for p in predictions]

            # M√©thode bas√©e sur la performance du mod√®le
            model_mape = performance.get('mape', 25) / 100
            model_mae = performance.get('mae', 2)

            lower_bounds = []
            upper_bounds = []

            for i, (demand, conf) in enumerate(zip(demands, confidences)):
                # Marge d'erreur bas√©e sur MAPE et MAE
                mape_margin = demand * model_mape * (1 - conf)
                mae_margin = model_mae * (1 - conf)

                # Marge temporelle (augmente avec la distance)
                temporal_margin = demand * 0.02 * i

                # Marge totale
                total_margin = mape_margin + mae_margin + temporal_margin

                # Ajustement bas√© sur la confiance
                confidence_adj = 1 + (1 - conf) * 0.5
                final_margin = total_margin * confidence_adj

                lower_bound = max(0, int(demand - final_margin))
                upper_bound = max(demand, int(demand + final_margin))

                lower_bounds.append(lower_bound)
                upper_bounds.append(upper_bound)

            avg_margin = np.mean([u - d for u, d in zip(upper_bounds, demands)])

            return {
                'lower': lower_bounds,
                'upper': upper_bounds,
                'margin': float(avg_margin),
                'method': 'performance_based',
                'confidence_level': 0.8,  # 80% d'intervalle de confiance
                'margin_components': {
                    'model_uncertainty': float(np.mean([d * model_mape for d in demands])),
                    'temporal_uncertainty': float(np.mean([d * 0.02 * i for i, d in enumerate(demands)])),
                    'confidence_adjustment': float(np.mean([1 + (1 - c) * 0.5 for c in confidences]))
                }
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur calcul intervalles am√©lior√©s: {e}")
            # Fallback simple
            demands = [p.get('predicted_demand', 5) for p in predictions]
            margins = [max(1, int(d * 0.25)) for d in demands]
            return {
                'lower': [max(0, d - m) for d, m in zip(demands, margins)],
                'upper': [d + m for d, m in zip(demands, margins)],
                'margin': float(np.mean(margins)),
                'method': 'fallback'
            }

    def calculate_enhanced_quality_metrics(self, predictions, performance, contextual_data):
        """Calculer des m√©triques de qualit√© am√©lior√©es"""
        try:
            if not predictions or not performance:
                return {
                    'prediction_confidence': 0.4,
                    'data_quality_score': 0.3,
                    'model_reliability': 0.3,
                    'overall_quality': 0.3
                }

            # Confiance moyenne des pr√©dictions
            pred_confidences = [p.get('confidence', 0.5) for p in predictions]
            avg_prediction_confidence = np.mean(pred_confidences)

            # Score de qualit√© du mod√®le bas√© sur MAPE
            model_mape = performance.get('mape', 50)
            model_quality = max(0.1, min(1.0, 1.0 - (model_mape / 100)))

            # Fiabilit√© bas√©e sur la stabilit√© (std des erreurs si disponible)
            if 'mape_std' in performance:
                stability = max(0.1, 1.0 - (performance['mape_std'] / 20))
            else:
                stability = 0.7  # Valeur par d√©faut

            # Score de compl√©tude des donn√©es
            data_completeness = contextual_data.get('completeness_score', 0.5)

            # Bonus pour des mod√®les avanc√©s
            model_type = performance.get('model_type', 'unknown')
            model_bonus = 1.0
            if model_type == 'ensemble':
                model_bonus = 1.2
            elif model_type in ['machine_learning', 'time_series_advanced']:
                model_bonus = 1.1

            # Nombre d'√©chantillons d'entra√Ænement
            training_samples = performance.get('training_samples', 30)
            sample_quality = min(1.0, training_samples / 100)

            # Calcul des m√©triques finales
            prediction_confidence = min(0.95, avg_prediction_confidence * model_bonus)
            data_quality_